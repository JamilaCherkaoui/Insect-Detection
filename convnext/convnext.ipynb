{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVfq6BdSyOfg",
        "outputId": "dd4274e9-33b3-459a-83d5-317592a54a10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mod\n"
      ],
      "metadata": {
        "id": "Zt2g13NCIYtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t-eeO6aKywTs",
        "outputId": "bea0f992-fcf9-4dcc-bc6f-4ca7c7ea5bcd",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "2e8f0461d6d947c0b46072a0b9d8940c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Pour MixUp / CutMix (inclus dans timm.data.mixup)\n",
        "from timm.data import Mixup\n",
        "from timm.loss import SoftTargetCrossEntropy\n"
      ],
      "metadata": {
        "id": "C7lkGzMpzlKw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "\n",
        "class ImageLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, image_filenames, transform=None, size=(300,300)):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_filenames = image_filenames\n",
        "        self.transform = transform\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.image_filenames[idx]\n",
        "        # Extraire label (même logique que chez vous)\n",
        "        vote = fname.split('-')[1]\n",
        "        label = int(vote.split('_')[0])\n",
        "\n",
        "        path = os.path.join(self.image_dir, fname)\n",
        "        img = Image.open(path).convert(\"RGB\").resize(self.size)\n",
        "\n",
        "        img = np.array(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "        return img, label\n",
        "\n",
        "# Exemple de transformations Albumentations\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.ElasticTransform(p=0.2),\n",
        "    A.GridDistortion(p=0.2),\n",
        "    A.OpticalDistortion(p=0.2),\n",
        "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),\n",
        "    A.ColorJitter(p=0.3),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BoE2gXkzwA2",
        "outputId": "e67867f0-14b3-42be-8f37-5ca63c28b2b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ade9d3441f57>:37: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "image_dir = \"/content/drive/MyDrive/base_sure_enrish\"\n",
        "all_images = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "train_files, val_files = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ImageLabelDataset(image_dir, train_files, transform=train_transform, size=(300,300))\n",
        "val_dataset   = ImageLabelDataset(image_dir, val_files,   transform=val_transform,   size=(300,300))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "PX3mrBcbz4Rl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 9\n",
        "model = timm.create_model(\n",
        "    'convnext_base',       # vous pouvez essayer convnext_large, convnext_tiny, etc.\n",
        "    pretrained=True,\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "645f84f6ee034174a95d1e783b03821c",
            "0fb4eb39a32d4ccc8a6a746e9bad42b9",
            "778e6e7ea2134030866c8c4fcb90799e",
            "e7037a9471014fbe9e769cfc484e0199",
            "a724eb97b3b14c45b7b947e8cecc6e39",
            "490d0277e1ae401796f247b786ec370f",
            "76ad502da13f465b82c80a0d48748037",
            "fd81b1252d294237bdbe822eba1ef8e0",
            "f9adf23f53e9447583dbcfff620788dd",
            "8f500f37e9ff46009f1bcf7977cf7973",
            "55ea3ad9d06e47eca8e2b259f519b829"
          ]
        },
        "id": "U6IKsopg0OgB",
        "outputId": "cea0c66f-7556-4aba-c34a-6a1e9c02253a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "645f84f6ee034174a95d1e783b03821c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "OJTlhmsc0Tcw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=0.2,      # alpha param pour MixUp\n",
        "    cutmix_alpha=1.0,     # alpha param pour CutMix\n",
        "    cutmix_minmax=None,\n",
        "    prob=1.0,             # probabilité d’appliquer MixUp/CutMix\n",
        "    switch_prob=0.5,      # probabilité de switch entre MixUp ou CutMix\n",
        "    mode='batch',\n",
        "    label_smoothing=0.0,\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n"
      ],
      "metadata": {
        "id": "ALybJLPN0W0a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = timm.loss.SoftTargetCrossEntropy()\n"
      ],
      "metadata": {
        "id": "7nLxtgDu0ZZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Option 1 : Cosine Annealing\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
        "\n",
        "# (ou Option 2 : OneCycleLR, etc.)\n"
      ],
      "metadata": {
        "id": "y9P6mutP0cFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "best_val_loss = float('inf')\n",
        "best_weights = None\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses   = []\n",
        "val_accs     = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n",
        "\n",
        "    # === Phase d'entraînement ===\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Appliquer MixUp / CutMix\n",
        "        mixed_images, mixed_labels = mixup_fn(images, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(mixed_images)\n",
        "        loss = criterion(outputs, mixed_labels)  # SoftTargetCrossEntropy attend des labels \"mélangés\"\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # === Phase de validation ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            # Pour la validation, on n'a pas de MixUp => on revient à du cross-entropy standard\n",
        "            # Si vous tenez absolument à réutiliser SoftTargetCE, vous pouvez one-hot vos labels\n",
        "            loss_ce = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            val_loss += loss_ce.item()\n",
        "\n",
        "            # Calcul de l'accuracy\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(accuracy)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {accuracy:.2f}%\")\n",
        "\n",
        "    # Scheduler update\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early stopping condition (basé sur la val_loss)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Charger les meilleurs poids\n",
        "model.load_state_dict(best_weights)\n",
        "print(\"Meilleur modèle chargé (val_loss min).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ldf8s2j0eud",
        "outputId": "9ba1dbb0-725c-4673-fcd3-2a50b9ac5f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [1/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [08:57<00:00,  6.98s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6419 | Val Loss: 0.9789 | Val Acc: 67.54%\n",
            "\n",
            "Epoch [2/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.2287 | Val Loss: 0.7548 | Val Acc: 79.34%\n",
            "\n",
            "Epoch [3/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.0427 | Val Loss: 0.7514 | Val Acc: 75.41%\n",
            "\n",
            "Epoch [4/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.0674 | Val Loss: 0.7077 | Val Acc: 76.39%\n",
            "\n",
            "Epoch [5/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9097 | Val Loss: 0.6757 | Val Acc: 80.98%\n",
            "\n",
            "Epoch [6/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7569 | Val Loss: 0.6550 | Val Acc: 80.66%\n",
            "\n",
            "Epoch [7/50]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7384 | Val Loss: 0.6149 | Val Acc: 82.62%\n",
            "\n",
            "Epoch [8/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7525 | Val Loss: 0.5877 | Val Acc: 82.30%\n",
            "\n",
            "Epoch [9/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6882 | Val Loss: 0.6553 | Val Acc: 81.31%\n",
            "\n",
            "Epoch [10/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6312 | Val Loss: 0.5912 | Val Acc: 82.30%\n",
            "\n",
            "Epoch [11/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5915 | Val Loss: 0.5736 | Val Acc: 82.95%\n",
            "\n",
            "Epoch [12/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5740 | Val Loss: 0.6310 | Val Acc: 81.31%\n",
            "\n",
            "Epoch [13/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5534 | Val Loss: 0.5764 | Val Acc: 83.28%\n",
            "\n",
            "Epoch [14/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5142 | Val Loss: 0.5756 | Val Acc: 82.62%\n",
            "\n",
            "Epoch [15/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5672 | Val Loss: 0.5837 | Val Acc: 82.95%\n",
            "\n",
            "Epoch [16/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5373 | Val Loss: 0.5856 | Val Acc: 84.59%\n",
            "\n",
            "Epoch [17/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4719 | Val Loss: 0.6077 | Val Acc: 82.62%\n",
            "\n",
            "Epoch [18/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5634 | Val Loss: 0.5838 | Val Acc: 83.28%\n",
            "\n",
            "Epoch [19/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5245 | Val Loss: 0.5845 | Val Acc: 83.61%\n",
            "\n",
            "Epoch [20/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5632 | Val Loss: 0.5869 | Val Acc: 83.28%\n",
            "\n",
            "Epoch [21/50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4352 | Val Loss: 0.5845 | Val Acc: 82.95%\n",
            "Early stopping triggered.\n",
            "Meilleur modèle chargé (val_loss min).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les meilleurs poids\n",
        "model.load_state_dict(best_weights)\n",
        "print(\"Meilleur modèle chargé (val_loss min).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_GAc161ClNH",
        "outputId": "8ba3545c-858d-4bf8-abbf-0b6ac763dfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meilleur modèle chargé (val_loss min).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_weights, \"convnext_best.pth\")\n",
        "print(\"Fichier convnext_best.pth sauvegardé\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XovQDAjvCp4y",
        "outputId": "eb0ccd84-e08a-4367-e536-4b86829e91f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier convnext_best.pth sauvegardé\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# [1] Si vous avez besoin de timm pour ConvNeXt\n",
        "import timm\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 📦 Dataset pour test (pas de labels dans les noms)\n",
        "# --------------------------------------------------\n",
        "class ImageDatasetWithoutLabels(Dataset):\n",
        "    def __init__(self, image_dir, image_filenames, transform=None, img_size=(300,300)):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_filenames = image_filenames\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, filename)\n",
        "        # On ouvre l'image, on convertit en RGB, on resize\n",
        "        image = Image.open(img_path).convert(\"RGB\").resize(self.img_size)\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, filename\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🔧 Préparation\n",
        "# --------------------------------------------------\n",
        "image_dir = \"/content/drive/MyDrive/CHALLANGE IA/datatest\"  # Chemin vers votre dossier contenant les images de test\n",
        "test_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "# Exemple de transformations (vous pouvez les ajuster)\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(300, 300),   # Si nécessaire, sinon vous pouvez redimensionner avant\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Device:\", device)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🔍 Chargement du modèle entraîné\n",
        "# --------------------------------------------------\n",
        "NUM_CLASSES = 9\n",
        "\n",
        "# Exemple: chargement d'un modèle ConvNeXt\n",
        "# (Assurez-vous d'utiliser le même modèle que lors de l'entraînement)\n",
        "model = timm.create_model(\n",
        "    'convnext_base',       # ou convnext_tiny / convnext_small / convnext_large...\n",
        "    pretrained=False,      # on charge nos poids entraînés, donc pas ImageNet\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n",
        "\n",
        "# Charger les poids\n",
        "model.load_state_dict(torch.load(\"convnext_best.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(\"✅ Modèle chargé\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 📤 Dataset et DataLoader\n",
        "# --------------------------------------------------\n",
        "test_dataset = ImageDatasetWithoutLabels(\n",
        "    image_dir=image_dir,\n",
        "    image_filenames=test_files,\n",
        "    transform=val_transform,\n",
        "    img_size=(300, 300)    # Ajustez si votre modèle attend 224x224 ou autre\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "print(\"✅ Test loader prêt\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🧠 Inférence + création de la soumission\n",
        "# --------------------------------------------------\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Prédictions: argmax des logits\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        for filename, pred in zip(filenames, preds):\n",
        "            # On enlève l'extension .jpg pour l'ID\n",
        "            img_id = os.path.splitext(filename)[0]\n",
        "            submission_rows.append((img_id, pred))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 💾 Export CSV\n",
        "# --------------------------------------------------\n",
        "submission_df = pd.DataFrame(submission_rows, columns=[\"idx\", \"gt\"])\n",
        "submission_df.to_csv(\"submission_convnext.csv\", index=False)\n",
        "print(\"✅ Fichier de prédiction généré : submission_convnext.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT9i7eO7CQM8",
        "outputId": "e4d47a8d-9897-4a25-fb7d-0751e247d16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Device: cuda\n",
            "✅ Modèle chargé\n",
            "✅ Test loader prêt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [01:24<00:00,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fichier de prédiction généré : submission_convnext.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------\n",
        "# 1) Geler une partie du modèle\n",
        "# ------------------------------------\n",
        "# Supposons que vous ayez un modèle 'model' déjà chargé + MixUp / CutMix configuré.\n",
        "# Par exemple, geler les 2/3 des couches:\n",
        "num_layers_to_freeze = int(len(list(model.parameters())) * 2/3)\n",
        "frozen_count = 0\n",
        "for param in model.parameters():\n",
        "    frozen_count += 1\n",
        "    if frozen_count <= num_layers_to_freeze:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# ------------------------------------\n",
        "# 2) Label smoothing\n",
        "# ------------------------------------\n",
        "# CrossEntropy avec label_smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# ------------------------------------\n",
        "# 3) Optimiseur + Scheduler Cosine\n",
        "# ------------------------------------\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
        "\n",
        "# ------------------------------------\n",
        "# 4) Boucle d'entraînement \"Phase 1\"\n",
        "# ------------------------------------\n",
        "EPOCHS = 20\n",
        "best_val_loss = float('inf')\n",
        "best_weights = None\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses   = []\n",
        "val_accs     = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n[Phase 1] Epoch [{epoch+1}/{EPOCHS}]\")\n",
        "\n",
        "    # === Phase d'entraînement ===\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # MixUp / CutMix\n",
        "        mixed_images, mixed_labels = mixup_fn(images, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(mixed_images)\n",
        "        loss = criterion(outputs, mixed_labels)  # label_smoothing + MixUp => OK\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # === Phase de validation ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            # Pour la validation, on ne fait pas de MixUp, on calcule la CE standard\n",
        "            loss_ce = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            val_loss += loss_ce.item()\n",
        "\n",
        "            # Accuracy\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(accuracy)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {accuracy:.2f}%\")\n",
        "\n",
        "    # Scheduler (Cosine)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_weights = copy.deepcopy(model.state_dict())\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered (Phase 1).\")\n",
        "            break\n",
        "\n",
        "# Charger les meilleurs poids de la phase 1\n",
        "model.load_state_dict(best_weights)\n",
        "print(\">>> Meilleur modèle (Phase 1) chargé.\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5) Défreeze TOUT le modèle => Phase 2 (fine-tuning)\n",
        "# ----------------------------------------------------\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# On définit un nouvel optim + scheduler (LR plus bas)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "scheduler2 = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
        "\n",
        "EPOCHS2 = 30\n",
        "best_val_loss2 = float('inf')\n",
        "best_weights2 = None\n",
        "patience_counter2 = 0\n",
        "\n",
        "for epoch in range(EPOCHS2):\n",
        "    print(f\"\\n[Phase 2] Epoch [{epoch+1}/{EPOCHS2}]\")\n",
        "\n",
        "    # === Train ===\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # MixUp / CutMix\n",
        "        mixed_images, mixed_labels = mixup_fn(images, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(mixed_images)\n",
        "        loss = criterion(outputs, mixed_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # === Val ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss_ce = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            val_loss += loss_ce.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {accuracy:.2f}%\")\n",
        "\n",
        "    scheduler2.step()\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss2:\n",
        "        best_val_loss2 = val_loss\n",
        "        best_weights2 = copy.deepcopy(model.state_dict())\n",
        "        patience_counter2 = 0\n",
        "    else:\n",
        "        patience_counter2 += 1\n",
        "        if patience_counter2 >= patience:\n",
        "            print(\"Early stopping triggered (Phase 2).\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_weights2)\n",
        "print(\">>> Meilleur modèle (Phase 2) chargé.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Sauvegarde du modèle final sur disque\n",
        "# ----------------------------------------\n",
        "torch.save(model.state_dict(), \"model_finetuned_mixup.pth\")\n",
        "print(\">>> Fichier model_finetuned_mixup.pth sauvegardé\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Visualisation des courbes (optionnel)\n",
        "# ----------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Train Loss (Phase 1)\")\n",
        "# NB : vous pouvez fusionner ou distinguer Phase 2, etc.\n",
        "plt.title(\"Évolution de la Loss en Phase 1\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rY_GnbtWDm8o",
        "outputId": "3e11ed4a-228a-4d21-8326-67bd4d1a3edd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Phase 1] Epoch [1/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:44<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.8309 | Val Loss: 1.0905 | Val Acc: 60.98%\n",
            "\n",
            "[Phase 1] Epoch [2/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4354 | Val Loss: 0.8567 | Val Acc: 77.05%\n",
            "\n",
            "[Phase 1] Epoch [3/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3318 | Val Loss: 0.8106 | Val Acc: 74.75%\n",
            "\n",
            "[Phase 1] Epoch [4/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:44<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3415 | Val Loss: 0.7857 | Val Acc: 77.38%\n",
            "\n",
            "[Phase 1] Epoch [5/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:44<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.2517 | Val Loss: 0.7384 | Val Acc: 78.03%\n",
            "\n",
            "[Phase 1] Epoch [6/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1110 | Val Loss: 0.7024 | Val Acc: 80.33%\n",
            "\n",
            "[Phase 1] Epoch [7/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1008 | Val Loss: 0.7019 | Val Acc: 79.67%\n",
            "\n",
            "[Phase 1] Epoch [8/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0801 | Val Loss: 0.6775 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 1] Epoch [9/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0650 | Val Loss: 0.6842 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 1] Epoch [10/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1153 | Val Loss: 0.6868 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 1] Epoch [11/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0315 | Val Loss: 0.6852 | Val Acc: 81.31%\n",
            "\n",
            "[Phase 1] Epoch [12/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:46<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0834 | Val Loss: 0.6876 | Val Acc: 80.33%\n",
            "\n",
            "[Phase 1] Epoch [13/20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:45<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0545 | Val Loss: 0.6830 | Val Acc: 80.00%\n",
            "Early stopping triggered (Phase 1).\n",
            ">>> Meilleur modèle (Phase 1) chargé.\n",
            "\n",
            "[Phase 2] Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0629 | Val Loss: 0.6744 | Val Acc: 81.31%\n",
            "\n",
            "[Phase 2] Epoch [2/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0969 | Val Loss: 0.6786 | Val Acc: 81.64%\n",
            "\n",
            "[Phase 2] Epoch [3/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0746 | Val Loss: 0.6832 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 2] Epoch [4/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0348 | Val Loss: 0.6956 | Val Acc: 80.00%\n",
            "\n",
            "[Phase 2] Epoch [5/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1134 | Val Loss: 0.6776 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 2] Epoch [6/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:13<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0106 | Val Loss: 0.6570 | Val Acc: 80.66%\n",
            "\n",
            "[Phase 2] Epoch [7/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9560 | Val Loss: 0.6608 | Val Acc: 80.33%\n",
            "\n",
            "[Phase 2] Epoch [8/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9947 | Val Loss: 0.6543 | Val Acc: 81.97%\n",
            "\n",
            "[Phase 2] Epoch [9/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9743 | Val Loss: 0.6590 | Val Acc: 81.31%\n",
            "\n",
            "[Phase 2] Epoch [10/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9927 | Val Loss: 0.6493 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 2] Epoch [11/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9520 | Val Loss: 0.6530 | Val Acc: 80.33%\n",
            "\n",
            "[Phase 2] Epoch [12/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9843 | Val Loss: 0.6353 | Val Acc: 82.30%\n",
            "\n",
            "[Phase 2] Epoch [13/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9892 | Val Loss: 0.6389 | Val Acc: 80.66%\n",
            "\n",
            "[Phase 2] Epoch [14/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9943 | Val Loss: 0.6390 | Val Acc: 80.98%\n",
            "\n",
            "[Phase 2] Epoch [15/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0720 | Val Loss: 0.6450 | Val Acc: 80.66%\n",
            "\n",
            "[Phase 2] Epoch [16/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9780 | Val Loss: 0.6455 | Val Acc: 80.33%\n",
            "\n",
            "[Phase 2] Epoch [17/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [02:14<00:00,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9732 | Val Loss: 0.6402 | Val Acc: 81.31%\n",
            "Early stopping triggered (Phase 2).\n",
            ">>> Meilleur modèle (Phase 2) chargé.\n",
            ">>> Fichier model_finetuned_mixup.pth sauvegardé\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWjBJREFUeJzt3XlcFPX/B/DX7MIu5y6HnMrlrYiIKKZ+88gTlTIzM8vbTvuZqX3TSlM7+HZYpplmeWdZeXVaaaakmYpI3geJgsilwC7nArvz+wPZXAHlWHYWeD0fj3nozs7OvHdY2Zczn3mPIIqiCCIiIiKJyKQugIiIiJo2hhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMkFU5fvw43njjDeTm5kpdChERWQjDCFmN9PR03H///fD29oazs7PU5RARkYUIbHpG1uKnn36CRqPBo48+KnUpRERkQTwyQlZj2LBhFg0igiBg4cKFZl3n+vXrIQgCLl++bNb11oW532dgYCAmTZpktvVR7QiCgOeee07qMojMgmGEJFX+5V3V9Ndff0ldYqXeeust7Ny5U+oyGoXAwECMGDFC6jKsxq2ff5lMBl9fXwwePBj79u2TurQ6++qrr/D444+jTZs2EAQB/fr1k7okshI2UhdABACLFy9GUFBQhfmtW7eWoJq7e+uttzB69GiMHDnSZP748eMxduxYKJVKaQqjRmHQoEGYMGECRFFEYmIiPv74Y9x333348ccfERkZKXV5tbZy5UocO3YM3bt3x40bN6Quh6wIwwhZhcjISHTr1k3qMupMLpdDLpdLXQY1cG3btsXjjz9ufPzggw+ic+fOWLp0aYMOI5s2bULz5s0hk8nQqVMnqcshK8LTNGT1SkpK4ObmhsmTJ1d4TqvVws7ODnPmzDHOy8jIwNSpU+Hl5QU7OzuEhoZiw4YNd93OpEmTEBgYWGH+woULIQiC8bEgCMjPz8eGDRuMh9PLx1BUNWbk448/RnBwMJRKJXx9fTF9+nTk5OSYLNOvXz906tQJZ86cQf/+/eHg4IDmzZvjnXfeuWvtAKDT6fDCCy/Aw8MDzs7OuP/++3H16tVKl01JScGUKVPg5eUFpVKJ4OBgrF27tlrbuV1WVhbmzJmDkJAQODk5QaVSITIyEn///Xet1leZ0tJSvP7662jVqhWUSiUCAwPx8ssvQ6fTmSwXGxuLIUOGoFmzZrC3t0dQUBCmTJlissyWLVsQHh4OZ2dnqFQqhISE4MMPP7xrDQaDAUuXLkVwcDDs7Ozg5eWFp556CtnZ2SbLlZ92OnDgACIiImBnZ4eWLVti48aNtX7/ISEhaNasGRITEys8t3PnTnTq1Mn4c/z5559Nnr9y5QqeffZZtGvXDvb29nB3d8fDDz9c4TNaUlKCRYsWoU2bNrCzs4O7uzv+85//YPfu3SbLnTt3DqNHj4abmxvs7OzQrVs3fPfdd9V6H35+fpDJ+LVDFfHICFkFjUaD69evm8wTBAHu7u6wtbXFgw8+iO3bt+OTTz6BQqEwLrNz507odDqMHTsWAFBYWIh+/fohISEBzz33HIKCgvDNN99g0qRJyMnJwfPPP1/nWjdt2oRp06YhIiICTz75JACgVatWVS6/cOFCLFq0CAMHDsQzzzyD8+fPY+XKlTh69CgOHjwIW1tb47LZ2dkYOnQoRo0ahTFjxmDr1q146aWXEBISctf/EU+bNg2ff/45xo0bh169emHv3r0YPnx4heXS09Nxzz33GAdAenh4YNeuXZg6dSq0Wi1mzpxZo/1x6dIl7Ny5Ew8//DCCgoKQnp6OTz75BH379sWZM2fg6+tbo/VV9d42bNiA0aNHY/bs2Th8+DCio6Nx9uxZ7NixA0BZCB08eDA8PDwwd+5cuLi44PLly9i+fbtxPbt378ajjz6KAQMG4O233wYAnD17FgcPHrzrZ+Opp57C+vXrMXnyZMyYMQOJiYn46KOPcPz48Qo/x4SEBIwePRpTp07FxIkTsXbtWkyaNAnh4eEIDg6u8fvPzs5GdnZ2hdOWBw4cwPbt2/Hss8/C2dkZy5Ytw0MPPYSkpCS4u7sDAI4ePYo///wTY8eORYsWLXD58mWsXLkS/fr1w5kzZ+Dg4ACg7HMaHR1t/GxrtVrExsYiLi4OgwYNAgCcPn0avXv3RvPmzTF37lw4Ojri66+/xsiRI7Ft2zY8+OCDNX5vRAAAkUhC69atEwFUOimVSuNyv/zyiwhA/P77701eP2zYMLFly5bGx0uXLhUBiJ9//rlxXnFxsdizZ0/RyclJ1Gq1xvkAxNdee834eOLEiWJAQECFGl977TXx9n8qjo6O4sSJE6t8P4mJiaIoimJGRoaoUCjEwYMHi3q93rjcRx99JAIQ165da5zXt29fEYC4ceNG4zydTid6e3uLDz30UIVt3So+Pl4EID777LMm88eNG1fhfU6dOlX08fERr1+/brLs2LFjRbVaLRYUFNxxWwEBASbvvaioyOS9iaIoJiYmikqlUly8ePEd11W+vuHDh1f5fPl7mzZtmsn8OXPmiADEvXv3iqIoijt27BABiEePHq1yXc8//7yoUqnE0tLSu9Z1qz/++EMEIG7evNlk/s8//1xhfkBAgAhAjImJMc7LyMgQlUqlOHv27LtuC4A4depUMTMzU8zIyBAPHz4sDhgwQAQgLlmyxGQ5hUIhJiQkGOf9/fffIgBx+fLlxnmV/TwPHTpU4bMWGhp6x5+DKIrigAEDxJCQELGoqMg4z2AwiL169RLbtGlz1/d2q+DgYLFv3741eg01XjxeRlZhxYoV2L17t8m0a9cu4/P33XcfmjVrhq+++so4Lzs7G7t378YjjzxinPfTTz/B29vb5BJhW1tbzJgxA3l5edi/f79l3tBNe/bsQXFxMWbOnGlyePqJJ56ASqXCjz/+aLK8k5OTyVgBhUKBiIgIXLp06Y7b+emnnwAAM2bMMJl/+1EOURSxbds2REVFQRRFXL9+3TgNGTIEGo0GcXFxNXqPSqXS+N70ej1u3LgBJycntGvXrsbrqkz5e5s1a5bJ/NmzZwOAcR+6uLgAAH744QeUlJRUui4XFxfk5+dXOPVwN9988w3UajUGDRpkss/Cw8Ph5OSE33//3WT5jh074t577zU+9vDwQLt27e76cyy3Zs0aeHh4wNPTEz169MDBgwcxa9asCj/PgQMHmhyV69y5M1Qqlcl27O3tjX8vKSnBjRs30Lp1a7i4uJj8fFxcXHD69GlcvHix0pqysrKwd+9ejBkzBrm5ucZ9cOPGDQwZMgQXL15ESkpKtd4f0e14moasQkRExB0HsNrY2OChhx7CF198AZ1OB6VSie3bt6OkpMQkjFy5cgVt2rSpcF66Q4cOxuctqXx77dq1M5mvUCjQsmXLCvW0aNHCZHwKALi6uuLEiRN33Y5MJqtwuuj27WZmZiInJwerV6/G6tWrK11XRkbGHbd1O4PBgA8//BAff/wxEhMTodfrjc+Vnyqoi/L3dvspCm9vb7i4uBj3Yd++ffHQQw9h0aJF+OCDD9CvXz+MHDkS48aNM17d9Oyzz+Lrr79GZGQkmjdvjsGDB2PMmDEYOnToHWu4ePEiNBoNPD09K33+9n3m7+9fYRlXV9cK40uq8sADD+C5556DIAhwdnZGcHAwHB0dKyxXne0UFhYiOjoa69atQ0pKCsRb+lxqNBrj3xcvXowHHngAbdu2RadOnTB06FCMHz8enTt3BlB26kkURcyfPx/z58+vtO6MjAw0b968Wu+R6FYMI9RgjB07Fp988gl27dqFkSNH4uuvv0b79u0RGhpqlvXfHgLK3frlWt+quhJHNFOjZIPBAAB4/PHHMXHixEqXKf/yqa633noL8+fPx5QpU/D666/Dzc0NMpkMM2fONG7PHKr6+dz6/NatW/HXX3/h+++/xy+//IIpU6ZgyZIl+Ouvv+Dk5ARPT0/Ex8fjl19+wa5du7Br1y6sW7cOEyZMuOMgZ4PBAE9PT2zevLnS5z08PEwe1/Xn2KJFCwwcOPCuy1VnO//3f/+HdevWYebMmejZsyfUajUEQcDYsWNNfj59+vTBP//8g2+//Ra//vorPvvsM3zwwQdYtWoVpk2bZlx2zpw5GDJkSKXbtdZL8cn6MYxQg9GnTx/4+Pjgq6++wn/+8x/s3bsXr7zyiskyAQEBOHHiBAwGg8nRkXPnzhmfr4qrq2uFK1yAyo+m3O2L8dZ6AOD8+fNo2bKlcX5xcTESExOr9YVT3e0YDAb8888/JkdDzp8/b7Jc+ZU2er3ebNveunUr+vfvjzVr1pjMz8nJQbNmzeq8/vL3dvHiReMRLqBsIG5OTk6Fn+k999yDe+65B2+++Sa++OILPPbYY9iyZQumTZsGoOyoVFRUFKKiomAwGPDss8/ik08+wfz586v8Mm3VqhX27NmD3r17m5z2aAi2bt2KiRMnYsmSJcZ5RUVFlX7Wy69amzx5MvLy8tCnTx8sXLgQ06ZNM35+bW1tzfbZISrHMSPUYMhkMowePRrff/89Nm3ahNLSUpNTNEBZS/m0tDSTsSWlpaVYvnw5nJyc0Ldv3yrX36pVK2g0GpNTIqmpqcarNW7l6OhY6S/z2w0cOBAKhQLLli0z+d/qmjVroNFoKr3apTbKr7RZtmyZyfylS5eaPJbL5XjooYewbds2nDp1qsJ6MjMza7xtuVxe4X/833zzjdnGDwwbNgxAxffy/vvvA4BxH2ZnZ1eoo0uXLgBgvAT49kZbMpnMeCTo9suEbzVmzBjo9Xq8/vrrFZ4rLS2t1mdBKpX9fJYvX17hiN/t+8bJyQmtW7c27hdPT0/069cPn3zyCVJTUytspzafHaJyPDJCVmHXrl3Goxe36tWrl8kRhUceeQTLly/Ha6+9hpCQEJP/KQPAk08+iU8++QSTJk3CsWPHEBgYiK1bt+LgwYNYunTpHe8GPHbsWLz00kt48MEHMWPGDBQUFGDlypVo27ZthYGY4eHh2LNnD95//334+voiKCgIPXr0qLBODw8PzJs3D4sWLcLQoUNx//334/z58/j444/RvXt3k8GqddGlSxc8+uij+Pjjj6HRaNCrVy/89ttvSEhIqLDs//73P/z+++/o0aMHnnjiCXTs2BFZWVmIi4vDnj17kJWVVaNtjxgxAosXL8bkyZPRq1cvnDx5Eps3bzb5ud1NQkIC3njjjQrzw8LCMHz4cEycOBGrV69GTk4O+vbtiyNHjmDDhg0YOXIk+vfvDwDYsGEDPv74Yzz44INo1aoVcnNz8emnn0KlUhkDzbRp05CVlYX77rsPLVq0wJUrV7B8+XJ06dKlwmfpVn379sVTTz2F6OhoxMfHY/DgwbC1tcXFixfxzTff4MMPP8To0aNrtN8sZcSIEdi0aRPUajU6duyIQ4cOYc+ePRXG83Ts2BH9+vVDeHg43NzcEBsbi61bt5rc/2bFihX4z3/+g5CQEDzxxBNo2bIl0tPTcejQIVy9evWuvWViYmIQExMDoCy85OfnG3/uffr0QZ8+fcz87qnBkOgqHiJRFO98aS8Acd26dSbLGwwG0c/PTwQgvvHGG5WuMz09XZw8ebLYrFkzUaFQiCEhIRXWI4oVL+0VRVH89ddfxU6dOokKhUJs166d+Pnnn1d6ae+5c+fEPn36iPb29iIA46Wut1/aW+6jjz4S27dvL9ra2opeXl7iM888I2ZnZ5ss07dvXzE4OLhCnVVdcny7wsJCccaMGaK7u7vo6OgoRkVFicnJyZW+z/T0dHH69Omin5+faGtrK3p7e4sDBgwQV69efdftVHZp7+zZs0UfHx/R3t5e7N27t3jo0CGxb9++1bp0s/xS2MqmqVOniqIoiiUlJeKiRYvEoKAg0dbWVvTz8xPnzZtncolpXFyc+Oijj4r+/v6iUqkUPT09xREjRoixsbHGZbZu3SoOHjxY9PT0FBUKhejv7y8+9dRTYmpq6l3rFEVRXL16tRgeHi7a29uLzs7OYkhIiPjf//5XvHbtmsn7qewS2eruDwDi9OnTa73c7T+f7Oxs478HJycncciQIeK5c+cqLPfGG2+IERERoouLi2hvby+2b99efPPNN8Xi4mKT9f/zzz/ihAkTRG9vb9HW1lZs3ry5OGLECHHr1q13rbn831Jl0+2fUWpaBFE008g4IiIiolrgmBEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQaRNMzg8GAa9euwdnZudptuImIiEhaoigiNzcXvr6+FW5geqsGEUauXbsGPz8/qcsgIiKiWkhOTkaLFi2qfL5BhJHyFt7JyclQqVQSV0NERETVodVq4efnd8dbcQANJIyUn5pRqVQMI0RERA3M3YZYcAArERERSYphhIiIiCTFMEJERESSahBjRoiIGiJRFFFaWgq9Xi91KUT1Qi6Xw8bGps5tNxhGiIjqQXFxMVJTU1FQUCB1KUT1ysHBAT4+PlAoFLVeB8MIEZGZGQwGJCYmQi6Xw9fXFwqFgg0bqdERRRHFxcXIzMxEYmIi2rRpc8fGZnfCMEJEZGbFxcUwGAzw8/ODg4OD1OUQ1Rt7e3vY2triypUrKC4uhp2dXa3WwwGsRET1pLb/SyRqSMzxOee/FCIiIpIUwwgRERFJimGEiIjqTWBgIJYuXSp1GWY1f/58PPnkk9Vefv369XBxcam/gurJ9evX4enpiatXr9b7thhGiIgIgiDccVq4cGGt1nv06NEafXFXpl+/fpg5c2ad1mEuaWlp+PDDD/HKK68Y502aNMm4nxQKBVq3bo3FixejtLRUwkrvbvXq1ejXrx9UKhUEQUBOTo7J882aNcOECRPw2muv1XstTTqM/HDiGl785m+cuJojdSlERJJKTU01TkuXLoVKpTKZN2fOHOOy5c3cqsPDw6NRXVH02WefoVevXggICDCZP3ToUKSmpuLixYuYPXs2Fi5ciHfffVeiKqunoKAAQ4cOxcsvv1zlMpMnT8bmzZuRlZVVr7U06TDy44lUfHPsKg4m3JC6FCJqxERRREFxqSSTKIrVqtHb29s4qdVqCIJgfHzu3Dk4Oztj165dCA8Ph1KpxIEDB/DPP//ggQcegJeXF5ycnNC9e3fs2bPHZL23n6YRBAGfffYZHnzwQTg4OKBNmzb47rvv6rR/t23bhuDgYCiVSgQGBmLJkiUmz3/88cdo06YN7Ozs4OXlhdGjRxuf27p1K0JCQmBvbw93d3cMHDgQ+fn5VW5ry5YtiIqKqjBfqVTC29sbAQEBeOaZZzBw4MAK7+uXX35Bhw4d4OTkZAwv5Y4ePYpBgwahWbNmUKvV6Nu3L+Li4ozPi6KIhQsXwt/fH0qlEr6+vpgxY4bxeZ1Ohzlz5qB58+ZwdHREjx49sG/fvjvut5kzZ2Lu3Lm45557qlwmODgYvr6+2LFjxx3XVVdNus9IeIArdp1Kw7Er2VKXQkSNWGGJHh0X/CLJts8sHgIHhXl+1c+dOxfvvfceWrZsCVdXVyQnJ2PYsGF48803oVQqsXHjRkRFReH8+fPw9/evcj2LFi3CO++8g3fffRfLly/HY489hitXrsDNza3GNR07dgxjxozBwoUL8cgjj+DPP//Es88+C3d3d0yaNAmxsbGYMWMGNm3ahF69eiErKwt//PEHgLKjQY8++ijeeecdPPjgg8jNzcUff/xRZYDLysrCmTNn0K1bt7vWZW9vjxs3/v2PbkFBAd577z1s2rQJMpkMjz/+OObMmYPNmzcDAHJzczFx4kQsX74coihiyZIlGDZsGC5evAhnZ2ds27YNH3zwAbZs2YLg4GCkpaXh77//Nq7/ueeew5kzZ7BlyxZjeBg6dChOnjyJNm3a1Hi/3ioiIgJ//PEHpk6dWqf13EmTDiNdA1wBAHFJ2RBFkR0SiYjuYPHixRg0aJDxsZubG0JDQ42PX3/9dezYsQPfffcdnnvuuSrXM2nSJDz66KMAgLfeegvLli3DkSNHMHTo0BrX9P7772PAgAGYP38+AKBt27Y4c+YM3n33XUyaNAlJSUlwdHTEiBEj4OzsjICAAISFhQEoCyOlpaUYNWqU8bRLSEhIldtKSkqCKIrw9fWtchlRFPHbb7/hl19+wf/93/8Z55eUlGDVqlVo1aoVgLLwsHjxYuPz9913n8l6Vq9eDRcXF+zfvx8jRoxAUlISvL29MXDgQNja2sLf3x8RERHGutatW4ekpCRjbXPmzMHPP/+MdevW4a233qr2/qyMr68vjh8/Xqd13E2TDiPBvioobGTIyi/G5RsFCGrmKHVJRNQI2dvKcWbxEMm2bS63HxHIy8vDwoUL8eOPPxq/2AsLC5GUlHTH9XTu3Nn4d0dHR6hUKmRkZNSqprNnz+KBBx4wmde7d28sXboUer0egwYNQkBAAFq2bImhQ4di6NChxlNEoaGhGDBgAEJCQjBkyBAMHjwYo0ePhqura6XbKiwsBIBKu4z+8MMPcHJyQklJCQwGA8aNG2cy6NfBwcEYRADAx8fH5D2np6fj1Vdfxb59+5CRkQG9Xo+CggLjvnz44YexdOlS4/sYNmwYoqKiYGNjg5MnT0Kv16Nt27YmNel0Ori7u9dsh1bC3t6+3u+x1KTDiNJGjs7N1Yi9ko24K9kMI0RULwRBMNupEik5Opr+jpwzZw52796N9957D61bt4a9vT1Gjx6N4uLiO67H1tbW5LEgCDAYDGavFwCcnZ0RFxeHffv24ddff8WCBQuwcOFCHD16FC4uLti9ezf+/PNP/Prrr1i+fDleeeUVHD58GEFBQRXW1axZMwBAdnY2PDw8TJ7r378/Vq5cCYVCAV9fX9jYmP68K3vPt54OmjhxIm7cuIEPP/wQAQEBUCqV6Nmzp3Ff+vn54fz589izZw92796NZ599Fu+++y7279+PvLw8yOVyHDt2DHK5afh0cnKq/c67KSsrq8L7NbcmPYAVKBs3AgDHkjhuhIioJg4ePIhJkybhwQcfREhICLy9vXH58mWL1tChQwccPHiwQl1t27Y1fjHb2Nhg4MCBeOedd3DixAlcvnwZe/fuBVAWCnr37o1Fixbh+PHjUCgUVQ7WbNWqFVQqFc6cOVPhOUdHR7Ru3Rr+/v4Vgkh1HDx4EDNmzMCwYcOMg3GvX79usoy9vT2ioqKwbNky7Nu3D4cOHcLJkycRFhYGvV6PjIwMtG7d2mTy9vaucS23O3XqlPHUVn1p+FG9jsL8b44b4SBWIqIaadOmDbZv346oqCgIgoD58+fX2xGOzMxMxMfHm8zz8fHB7Nmz0b17d7z++ut45JFHcOjQIXz00Uf4+OOPAZSdPrl06RL69OkDV1dX/PTTTzAYDGjXrh0OHz6M3377DYMHD4anpycOHz6MzMxMdOjQodIaZDIZBg4ciAMHDmDkyJFmfX9t2rTBpk2b0K1bN2i1Wrz44ouwt7c3Pr9+/Xro9Xr06NEDDg4O+Pzzz2Fvb4+AgAC4u7vjsccew4QJE7BkyRKEhYUhMzMTv/32Gzp37ozhw4dXus20tDSkpaUhISEBAHDy5Ek4OzvD39/fOJi4oKAAx44dq/O4k7tp8kdGuga4AADOp+dCW1QibTFERA3I+++/D1dXV/Tq1QtRUVEYMmQIunbtWi/b+uKLLxAWFmYyffrpp+jatSu+/vprbNmyBZ06dcKCBQuwePFiTJo0CQDg4uKC7du347777kOHDh2watUqfPnllwgODoZKpUJMTAyGDRuGtm3b4tVXX8WSJUsQGRlZZR3Tpk3Dli1bzB661qxZg+zsbHTt2hXjx4/HjBkz4OnpaXzexcUFn376KXr37o3OnTtjz549+P77741jQtatW4cJEyZg9uzZaNeuHUaOHImjR4/e8aqmVatWISwsDE888QQAoE+fPggLCzO5JPnbb7+Fv78/7r33XrO+39sJYnUvQpeQVquFWq2GRqOBSqUy+/r7vPM7krIKsHFKBPq0rd/zYkTU+BUVFSExMRFBQUG1vqU6WSdRFNGjRw+88MILxiuCGrN77rkHM2bMwLhx46pc5k6f9+p+fzf5IyPALeNGeKqGiIjuQBAErF692upbvZvD9evXMWrUKIuErhqHkZiYGERFRcHX1xeCIGDnzp13fc3mzZsRGhoKBwcH+Pj4YMqUKSbNYKR2a78RIiKiO+nSpQvGjx8vdRn1rlmzZvjvf/9rkR5cNQ4j+fn5CA0NxYoVK6q1/MGDBzFhwgRMnToVp0+fxjfffIMjR44Yz1FZg/Cbg1jjk3KgN1j9WSsiIqJGpcZX00RGRt5xcM/tDh06hMDAQGMP/aCgIDz11FN4++23q3yNTqeDTqczPtZqtTUts0baeTvDUSFHrq4UFzNy0d7b/ONSiIiIqHL1PmakZ8+eSE5Oxk8//QRRFJGeno6tW7di2LBhVb4mOjoaarXaOPn5+dVrjXKZgC7+LgA4boSIzKcBXB9AVGfm+JzXexjp3bs3Nm/ejEceeQQKhcJ4R8g7neaZN28eNBqNcUpOTq7vMo2nahhGiKiuyrtt1ncLbSJrUP45v73LbE3Ue9OzM2fO4Pnnn8eCBQswZMgQpKam4sUXX8TTTz+NNWvWVPoapVIJpVJZ36WZMA5iZRghojqSy+VwcXEx3nvEwcGBN+KkRkcURRQUFCAjIwMuLi4VWtHXRL2HkejoaPTu3RsvvvgigLIbJDk6OuLee+/FG2+8AR8fn/ouoVrKO7FevlGA63k6NHOybBgiosalvA13bW8AR9RQuLi41LntfL2HkYKCggp9+svTkzWdT1Xb26KtlxMupOch7ko2BgfXvZ8/ETVdgiDAx8cHnp6eKClhd2dqnGxtbet0RKRcjcNIXl6esY89ACQmJiI+Ph5ubm7w9/fHvHnzkJKSgo0bNwIAoqKi8MQTT2DlypXG0zQzZ85EREQEfH196/wGzCk8wLUsjCTlMIwQkVnI5XKz/LImasxqPIA1NjbWeF8AAJg1axbCwsKwYMECAEBqaiqSkpKMy0+aNAnvv/8+PvroI3Tq1AkPP/ww2rVrh+3bt5vpLZhPV940j4iIyOJ4b5pb/JOZhwFL9kNpI8PJhUOgsGG3fCIiotrivWlqoWUzR7g42EJXasCZ1PpttEZERERlGEZuIQgC+40QERFZGMPIbdhvhIiIyLIYRm4THsAjI0RERJbEMHKb0BYukMsEpGmLcC2nUOpyiIiIGj2GkdvYK+QI9i0b8cujI0RERPWPYaQSXTmIlYiIyGIYRiphHMSaxDBCRERU3xhGKlE+iPX0NS0KikslroaIiKhxYxiphK/aDt4qO+gNIk5c1UhdDhERUaPGMFIJQRB4iS8REZGFMIxUoXzcyHGOGyEiIqpXDCNVuPXISAO4lyAREVGDxTBShY4+KihtZMguKEHi9XypyyEiImq0GEaqoLCRoXMLNQCOGyEiIqpPDCN3wH4jRERE9Y9h5A7C2YmViIio3jGM3EH5kZEL6XnQFJZIXA0REVHjxDByB82clAh0dwAAxCfnSFsMERFRI8Uwchdd2fyMiIioXjGM3EV5v5E4hhEiIqJ6wTByF139/+3Eqjew+RkREZG5MYzcRVsvZzgpbZBfrMf5tFypyyEiImp0GEbuQi4TEObvAgA4xn4jREREZscwUg3lp2o4boSIiMj8GEaqIZxX1BAREdUbhpFq6OLvAkEAkrIKkJmrk7ocIiKiRoVhpBpUdrZo5+UMgPepISIiMjeGkWoK47gRIiKiesEwUk0cN0JERFQ/GEaqqTyMnEjRQFeql7gaIiKixqPGYSQmJgZRUVHw9fWFIAjYuXPnHZefNGkSBEGoMAUHB9e2ZkkEujvAzVGB4lIDTl/TSl0OERFRo1HjMJKfn4/Q0FCsWLGiWst/+OGHSE1NNU7Jyclwc3PDww8/XONipSQIAvuNEBER1QObmr4gMjISkZGR1V5erVZDrVYbH+/cuRPZ2dmYPHlyTTctufAAV+w5m84raoiIiMyoxmGkrtasWYOBAwciICCgymV0Oh10un/7eWi11nFa5NZBrKIoQhAEiSsiIiJq+Cw6gPXatWvYtWsXpk2bdsfloqOjjUdU1Go1/Pz8LFThnXVuoYaNTEC6VoeUnEKpyyEiImoULBpGNmzYABcXF4wcOfKOy82bNw8ajcY4JScnW6bAu7CzlSPYVwWAl/gSERGZi8XCiCiKWLt2LcaPHw+FQnHHZZVKJVQqlclkLboGcBArERGROVksjOzfvx8JCQmYOnWqpTZZL4zjRjiIlYiIyCxqPIA1Ly8PCQkJxseJiYmIj4+Hm5sb/P39MW/ePKSkpGDjxo0mr1uzZg169OiBTp061b1qCZWHkbOpuSgoLoWDwuJjgImIiBqVGh8ZiY2NRVhYGMLCwgAAs2bNQlhYGBYsWAAASE1NRVJSkslrNBoNtm3b1uCPigCAj9oevmo76A0i/k7WSF0OERFRg1fj/9b369cPoihW+fz69esrzFOr1SgoKKjppqxW1wBXXDuRirikbPRs5S51OURERA0a701TC+WdWHlFDRERUd0xjNRC+biRuKRsGAxVHyUiIiKiu2MYqYWOvirY2cqQU1CCS9fzpS6HiIioQWMYqQVbuQydW7gAYL8RIiKiumIYqaVb71NDREREtccwUkvh/v+OGyEiIqLaYxippfK28Bcz8qApKJG4GiIiooaLYaSW3BwVaNnMEQAQl8yjI0RERLXFMFIHYf68aR4REVFdMYzUAQexEhER1R3DSB2Uh5H45ByU6g0SV0NERNQwMYzUQRtPJzgrbVBQrMe5tFypyyEiImqQGEbqQCYTEHbz6MhxXuJLRERUKwwjdRTOm+YRERHVCcNIHRkHsfLICBERUa0wjNRRqJ8aggAkZxUiQ1skdTlEREQNDsNIHTnb2aKdlzMAtoYnIiKqDYYRM2C/ESIiotpjGDEDhhEiIqLaYxgxg/IwcipFC12pXuJqiIiIGhaGETPwd3NAMycFivUGnErRSl0OERFRg8IwYgaCIKArb5pHRERUKwwjZtKV40aIiIhqhWHETG5tfiaKosTVEBERNRwMI2YS0lwNW7mAzFwdrmYXSl0OERFRg8EwYiZ2tnIE+6oB8FQNERFRTTCMmFH5qRp2YiUiIqo+hhEzYvMzIiKimmMYMaPyMHI2VYt8XanE1RARETUMDCNm5KWyQ3MXexhE4O/kHKnLISIiahAYRsyM/UaIiIhqpsZhJCYmBlFRUfD19YUgCNi5c+ddX6PT6fDKK68gICAASqUSgYGBWLt2bW3qtXrh/i4AyvqNEBER0d3Z1PQF+fn5CA0NxZQpUzBq1KhqvWbMmDFIT0/HmjVr0Lp1a6SmpsJgMNS42IYgPMANQFlbeINBhEwmSFwRERGRdatxGImMjERkZGS1l//555+xf/9+XLp0CW5uZV/UgYGBNd1sg9Hexxn2tnJoi0px6XoeWns6S10SERGRVav3MSPfffcdunXrhnfeeQfNmzdH27ZtMWfOHBQWVt2lVKfTQavVmkwNha1chlA/Nj8jIiKqrnoPI5cuXcKBAwdw6tQp7NixA0uXLsXWrVvx7LPPVvma6OhoqNVq4+Tn51ffZZoV+40QERFVX72HEYPBAEEQsHnzZkRERGDYsGF4//33sWHDhiqPjsybNw8ajcY4JScn13eZZtXVn2GEiIioumo8ZqSmfHx80Lx5c6jVauO8Dh06QBRFXL16FW3atKnwGqVSCaVSWd+l1Zuwm2Hkn8x8ZOcXw9VRIXFFRERE1qvej4z07t0b165dQ15ennHehQsXIJPJ0KJFi/revCTcHBVo6eEIADiezKMjREREd1LjMJKXl4f4+HjEx8cDABITExEfH4+kpCQAZadYJkyYYFx+3LhxcHd3x+TJk3HmzBnExMTgxRdfxJQpU2Bvb2+ed2GFwnmqhoiIqFpqHEZiY2MRFhaGsLAwAMCsWbMQFhaGBQsWAABSU1ONwQQAnJycsHv3buTk5KBbt2547LHHEBUVhWXLlpnpLVgn4x18r+RIWwgREZGVE0RRFKUu4m60Wi3UajU0Gg1UKpXU5VTLxfRcDPogBva2cpxcOBg2cnbeJyKipqW639/8hqwnrTycoLKzQWGJHufScqUuh4iIyGoxjNQTmUzgTfOIiIiqgWGkHrHfCBER0d0xjNQjdmIlIiK6O4aRehTq5wKZAKTkFCJdWyR1OURERFaJYaQeOSlt0N67bPRwHI+OEBERVYphpJ7xVA0REdGdMYzUM2MYSWIYISIiqgzDSD0rDyOnUjQoKtFLXA0REZH1YRipZy1c7dHMSYkSvYhTKRqpyyEiIrI6DCP1TBAEhAe4AOC4ESIiosowjFgAB7ESERFVjWHEAox38E3KQQO4LyEREZFFMYxYQLCvGgq5DNfzdEjOKpS6HCIiIqvCMGIBdrZydGpe1vzsWFKWxNUQERFZF4YRC+G4ESIiosoxjFjIv3fwzZG2ECIiIivDMGIhXW8eGTmfpkVuUYnE1RAREVkPhhEL8VLZoYWrPQwi8Hcym58RERGVYxixoH8v8eW4ESIionIMIxbEQaxEREQVMYxYUPkg1rikbBgMbH5GREQEMIxYVHtvZzgo5MgtKkVCZp7U5RAREVkFhhELspHLENrCBQBP1RAREZVjGLEwjhshIiIyxTBiYcYrahhGiIiIADCMWFyYvwsA4NL1fGTlF0tbDBERkRVgGLEwFwcFWns6AQCOs98IERERw4gUwv05boSIiKgcw4gEOIiViIjoXwwjEii/ad7fV3NQojdIXA0REZG0ahxGYmJiEBUVBV9fXwiCgJ07d95x+X379kEQhApTWlpabWtu8Fo2c4Ta3hZFJQacTdVKXQ4REZGkahxG8vPzERoaihUrVtTodefPn0dqaqpx8vT0rOmmGw2ZTEDXm1fV8FQNERE1dTY1fUFkZCQiIyNrvCFPT0+4uLjU+HWNVXiAK34/n4m4pBxM7i11NURERNKx2JiRLl26wMfHB4MGDcLBgwfvuKxOp4NWqzWZGpuubH5GREQEwAJhxMfHB6tWrcK2bduwbds2+Pn5oV+/foiLi6vyNdHR0VCr1cbJz8+vvsu0uNAWLpDLBKTkFCJVUyh1OURERJIRRFGs9b3sBUHAjh07MHLkyBq9rm/fvvD398emTZsqfV6n00Gn0xkfa7Va+Pn5QaPRQKVS1bZcqzNi+R84laLFinFdMbyzj9TlEBERmZVWq4Varb7r97ckl/ZGREQgISGhyueVSiVUKpXJ1Bix+RkREZFEYSQ+Ph4+PjwSUD5u5BjbwhMRURNW46tp8vLyTI5qJCYmIj4+Hm5ubvD398e8efOQkpKCjRs3AgCWLl2KoKAgBAcHo6ioCJ999hn27t2LX3/91XzvooHqevPIyOkUDYpK9LCzlUtcERERkeXVOIzExsaif//+xsezZs0CAEycOBHr169HamoqkpKSjM8XFxdj9uzZSElJgYODAzp37ow9e/aYrKOpauFqD09nJTJydTiZokH3QDepSyIiIrK4Og1gtZTqDoBpiJ75/Bh2nUrD3Mj2eLpvK6nLISIiMhurHsBK/+JN84iIqKljGJHYrc3PGsBBKiIiIrNjGJFYsK8KChsZbuQX48qNAqnLISIisjiGEYkpbeQIaa4GwFM1RETUNDGMWIFw9hshIqImjGHECpT3G+FN84iIqCliGLECXQNcAADn03ORW1QibTFEREQWxjBiBTyd7eDv5gBRBOKTc6Quh4iIyKIYRqwE+40QEVFTxTBiJboyjBARURPFMGIluvq7AADik3KgN7D5GRERNR0MI1ainZczHBVy5OpKcTEjV+pyiIiILIZhxErYyGXocvPoSNyVHElrISIisiSGESsS7s9xI0RE1PQwjFgR403z2ImViIiaEIYRKxJ288hI4vV83MjTSVwNERGRZTCMWBG1vS3aejkBAOKScqQthoiIyEIYRqxMV44bISKiJoZhxMoYx40wjBARURPBMGJlytvCxyfn4FSKRuJqiIiI6h/DiJVp2cwR/dp5oFhvwJT1R3Etp1DqkoiIiOoVw4iVEQQByx4NQzsvZ2Tk6jBl/VHkFpVIXRYREVG9YRixQio7W6yd3B0ezkqcS8vFs5vjUKI3SF0WERFRvWAYsVLNXeyxdmJ32NvK8cfF61jw7WmIIm+gR0REjQ/DiBULaaHGskfDIAjAl0eS8EnMJalLIiIiMjuGESs3qKMXFozoCAD4365z+PFEqsQVERERmRfDSAMwuXcQJvUKBAC88HU8G6IREVGjwjDSQMwf0REDO3iiuNSAJzbG4sqNfKlLIiIiMguGkQZCLhPw4dgwdGquQlZ+MSavP4qcgmKpyyIiIqozhpEGxFFpg7UTu8NXbYdLmfl4ctMx6Er1UpdFRERUJwwjDYynyg5rJ3eHk9IGRxKzMG/bSV7yS0REDVqNw0hMTAyioqLg6+sLQRCwc+fOar/24MGDsLGxQZcuXWq6WbpFe28VPn6sK+QyAduPp2DpnotSl0RERFRrNQ4j+fn5CA0NxYoVK2r0upycHEyYMAEDBgyo6SapEn3aeuDNkZ0AAB/+dhHbjl2VuCIiIqLasanpCyIjIxEZGVnjDT399NMYN24c5HJ5jY6mUNXGRvjjSlYBVu77B3O3n4Cviz16tnKXuiwiIqIasciYkXXr1uHSpUt47bXXqrW8TqeDVqs1mahyLw5uh+GdfVCiF/HUplgkZORKXRIREVGN1HsYuXjxIubOnYvPP/8cNjbVOxATHR0NtVptnPz8/Oq5yoZLJhOw5OFQdPV3gbaoFJPXH8X1PJ3UZREREVVbvYYRvV6PcePGYdGiRWjbtm21Xzdv3jxoNBrjlJycXI9VNnx2tnJ8OqEb/N0ckJxViGkbYlFUwkt+iYioYRDEOlwXKggCduzYgZEjR1b6fE5ODlxdXSGXy43zDAYDRFGEXC7Hr7/+ivvuu++u29FqtVCr1dBoNFCpVLUtt9H7JzMPoz7+E5rCEkR28saKcV0hkwlSl0VERE1Udb+/6/XIiEqlwsmTJxEfH2+cnn76abRr1w7x8fHo0aNHfW6+yWnl4YTV48OhkMuw61Qa3v75nNQlERER3VWNr6bJy8tDQkKC8XFiYiLi4+Ph5uYGf39/zJs3DykpKdi4cSNkMhk6depk8npPT0/Y2dlVmE/m0aOlO94Z3Rkzv4rHJzGX4O/ugMd6BEhdFhERUZVqfGQkNjYWYWFhCAsLAwDMmjULYWFhWLBgAQAgNTUVSUlJ5q2SamRkWHO8MLBsjM6Cb09j3/kMiSsiIiKqWp3GjFgKx4zUnCiKmPPNCWyLuwpHhRzfPN0LHX2574iIyHKsYswISUcQBESPCkHPlu7IL9ZjyvqjSNMUSV0WERFRBQwjjZjCRoZVj4ejlYcj0rRFmLL+KPJ0pVKXRUREZIJhpJFTO9hi/eQINHNS4EyqFv/3RRxK9QapyyIiIjJiGGkC/Nwc8OmEblDayPD7+Uws/uEMGsBQISIiaiIYRpqIMH9XLH2kCwQB2HjoCtYcSJS6JCIiIgAMI01KZIgP5kW2BwC8+dNZ/HwqTeKKiIiIGEaanCfubYnHevhDFIGZXx1HfHKO1CUREVETxzDSxAiCgEX3B6NfOw8UlRgwbcNRJGcVSF0WERE1YQwjTZCNXIaPxnVFe29nXM8rxuT1R6EpLJG6LCIiaqIYRpooJ6UN1k3uDi+VEgkZeXjm82MoLuUlv0REZHkMI02Yj9oeayZ2h4NCjj//uYFXdpzkJb9ERGRxDCNNXKfmaqwY1xUyAfjm2FWs+D3h7i8iIiIyI4YRQv/2nlh0fzAA4L1fL+Db+BSJKyIioqaEYYQAAON7BmLaf4IAAC9+cwJHErMkroiIiJoKhhEyenlYBwwJ9kKx3oAnN8XiUmae1CUREVETwDBCRjKZgKWPhCG0hRo5BSWYsv4osvKLpS6LiIgaOUFsAJdPaLVaqNVqaDQaqFQqqctp9DJzdRi54iBScgoRHuCKzdN6wM5WLlk9RSV6ZGh1SNUUIk1bhFRNEdI0RTcf69DC1R6L7w+Gu5NSshqJiKii6n5/M4xQpS6m52LUyj+RW1SKEZ19sGxsGGQywezbydeVmoSLdJOwUYQ0bVG1js6093bGF0/cAzdHhdlrJCKi2mEYoTo7mHAdE9ceQalBxPT+rfDikPbVfq0oitAWliJVW2gaLm4ezUjTlM3PLSqt1vrsbGXwUdvDW2UHb3XZ5KO2g9reFm/8eBaZuTq093bGl0/cA1cGEiIiq8AwQmbxTWwyXtx6AgDwzkOdMaa7HwwGETfyi+94NCNVU4iikup1dHVW2pgEDG+1fdmfKtPQIQiVH5lJyMjD2NV/4XqeDh18VPhiWg8GEiIiK8AwQmaz5NfzWL43ATYyAd5qO6Rri1Cir97Hxs1RAW9VWaDwUtvBxxgw7I0BxElpU+caEzJyMXb1YVzP06GjjwqbGUiIiCTHMEJmI4oiZn4Vj2/jrxnnCQLg4aS8eSTj33Dho7aDl+rfPy058PViei4e/fQvXM8rRrBvWSBxcWAgISKSCsMImVWp3oA//7kBR6UNfNR28HBWwlZufVeGM5AQEVmP6n5/W9+3CVklG7kMfdp6IDzAFb4u9lYZRACgjVfZVTXujgqcvqbF42sOI6eAvVKIiKyZdX6jENVBWy9nfPlkWSA5laLF+DVHoCkokbosIiKqAsMINUptvf7tO3IyRYPxaw9DU8hAQkRkjRhGqNFq5+2ML57oATdHBU5c1WD8GgYSIiJrxDBCjVp775uX+TrY4sRVDSYwkBARWR2GEWr0Ovio8MUT98DVwRZ/X9Vgwtoj0BYxkBARWQuGEWoSOviosHnazUCSnIMJaxhIiIisBcMINRkdfcsCiYuDLeKTczBx7RHkMpAQEUmuxmEkJiYGUVFR8PX1hSAI2Llz5x2XP3DgAHr37g13d3fY29ujffv2+OCDD2pbL1GddLzZCE1tb4vjSTmYwEBCRCS5GoeR/Px8hIaGYsWKFdVa3tHREc899xxiYmJw9uxZvPrqq3j11VexevXqGhdLZA7BvmqTQMIjJERE0qpTO3hBELBjxw6MHDmyRq8bNWoUHB0dsWnTpmotz3bwVB9OpWjw2GdlV9eEB7hiw5QIs9y0j4iIylhtO/jjx4/jzz//RN++fatcRqfTQavVmkxE5tapedkREpWdDY5dycaktUeQpyuVuiwioibHYmGkRYsWUCqV6NatG6ZPn45p06ZVuWx0dDTUarVx8vPzs1SZ1MSUBZJ7oLKzQeyVbExex0BCRGRpFgsjf/zxB2JjY7Fq1SosXboUX375ZZXLzps3DxqNxjglJydbqkxqgkJaqPH5tB5wtrPB0ctlgSSfgYSIyGIsFkaCgoIQEhKCJ554Ai+88AIWLlxY5bJKpRIqlcpkIqpPnVu44POptwaSowwkREQWIkmfEYPBAJ1OJ8WmiaoU6ueCTVN7wFlpgyOXszB5PQMJEZEl1DiM5OXlIT4+HvHx8QCAxMRExMfHIykpCUDZKZYJEyYYl1+xYgW+//57XLx4ERcvXsSaNWvw3nvv4fHHHzfPOyAyoy5+Ltg07WYgSczClPVHUVDMQEJEVJ9qfB1jbGws+vfvb3w8a9YsAMDEiROxfv16pKamGoMJUHYUZN68eUhMTISNjQ1atWqFt99+G0899ZQZyicyvy5+Ltg4NQIT1hzB4ZuBZO2k7nBQ8LJfIqL6UKc+I5bCPiMkhbikbExYU3Z1Tc+W7lg7qTvsFXKpyyIiajCsts8IUUPR1f/fRmiHLt3AlPVHUVisl7osIqJGh2GE6A7KOrN2h6NCjkOXbmDqBgYSIiJzYxghuovwADdsnBoBR4Ucf/5zA9M2HkVRCQMJEZG5MIwQVUN4gBs2TCkLJAcTbmDahlgGEiIiM2EYIaqmboFuWD8lAg4KOQ4kXMcTGxlIiIjMgWGEqAa6B7ph/eSyQPLHRQYSIiJzYBghqqGIIDesm9TdGEie3HSMgYSIqA4YRohqoUdLd6yb1B32tnLEXMhkICEiqgOGEaJa6tHSHesm/xtInmIgISKqFYYRojq4p7wzq60c+y9k4pnPj0FXykBCRFQTDCNEddSzVVkgsbOV4ffzmXh6EwMJEVFNMIwQmUHPVu5YO/HfQPLM53EMJERE1cQwQmQmvVo3w5qJ3aG0kWHvuQzM235S6pKIiBoEhhEiM+p9M5DIBGB7XAp+P58hdUlERFaPYYTIzP7Tphkm9w4CALy64xTydaUSV0REZN0YRojqwaxBbdHcxR4pOYX4YPcFqcshIrJqDCNE9cBRaYM3HuwEAFh7MBEnruZIWxARkRVjGCGqJ/3beeL+UF8YRGDutpMo0RukLomIyCoxjBDVowVRHeHiYIszqVqsPZAodTlERFaJYYSoHjVzUuKVYR0AAB/suYCkGwUSV0REZH0YRojq2ejwFujVyh1FJQa8vOMkRFGUuiQiIqvCMEJUzwRBwFsPhkBpI8OBhOvYcTxF6pKIiKwKwwiRBQQ2c8TzA9sAAF7/4Qxu5OkkroiIyHowjBBZyBP3tkR7b2dkF5TgjR/PSl0OEZHVYBghshBbuQz/e6gzBAHYcTwFMRcypS6JiMgqMIwQWVAXPxdM6hUIAHhl50kUFLNVPBERwwiRhc0e3A6+ajskZxVi6Z6LUpdDRCQ5hhEiC3O6pVX8Z39cwqkUjcQVERFJi2GESAL3tffCiM4+Za3it59AKVvFE1ETxjBCJJEFUR2hsrPBqRQt1h28LHU5RESSYRghkoinsx1eGV7WKv793ReQnMVW8UTUNNU4jMTExCAqKgq+vr4QBAE7d+684/Lbt2/HoEGD4OHhAZVKhZ49e+KXX36pbb1EjcqYbn64p6UbCkv0eGXnKbaKJ6ImqcZhJD8/H6GhoVixYkW1lo+JicGgQYPw008/4dixY+jfvz+ioqJw/PjxGhdL1NiUt4pX2MgQcyET38Zfk7okIiKLE8Q6/FdMEATs2LEDI0eOrNHrgoOD8cgjj2DBggXVWl6r1UKtVkOj0UClUtWiUiLr9tHei3jv1wtwc1Rgz6y+cHNUSF0SEVGdVff72+JjRgwGA3Jzc+Hm5lblMjqdDlqt1mQiasye7NMK7byckZVfjDfZKp6ImhiLh5H33nsPeXl5GDNmTJXLREdHQ61WGyc/Pz8LVkhkeQobGaIfCoEgANviruLAxetSl0REZDEWDSNffPEFFi1ahK+//hqenp5VLjdv3jxoNBrjlJycbMEqiaTR1d8VE+4JAAC8vOMkCov1EldERGQZFgsjW7ZswbRp0/D1119j4MCBd1xWqVRCpVKZTERNwYtD28NHbYekrAJ8+BtbxRNR02CRMPLll19i8uTJ+PLLLzF8+HBLbJKoQXJS2uD1B8paxX/6xyWcvsZW8UTU+NU4jOTl5SE+Ph7x8fEAgMTERMTHxyMpKQlA2SmWCRMmGJf/4osvMGHCBCxZsgQ9evRAWloa0tLSoNHwlyxRZQZ29MKwEG/oDSLmbT8JvYG9R4iocatxGImNjUVYWBjCwsIAALNmzUJYWJjxMt3U1FRjMAGA1atXo7S0FNOnT4ePj49xev755830Fogan4VRwXC2s8GJqxqs//Oy1OUQEdWrOvUZsRT2GaGm6MsjSZi3/SQcFHL8+kIftHB1kLokIqIasdo+I0RUPY9080NEoBsKivV4la3iiagRYxghslIymYC3RoVAIZdh3/lMfH8iVeqSiIjqBcMIkRVr7emE5+5rDQBY/P1p5BQUS1wREZH5MYwQWbmn+7ZCG08nXM9jq3giapwYRoisnMJGhv/dbBX/zbGr+DOBreKJqHFhGCFqAMID3PB4j39bxReVsFU8ETUeDCNEDcSLQ9vBS6XE5RsFWMZW8UTUiDCMEDUQKjtbLL7ZKn51zCWcTdVKXBERkXkwjBA1IEOCvTE02BulBhFz2SqeiBoJhhGiBmbRA8FwVtrg7+QcbDx0WepyiIjqjGGEqIHxUtnhpcj2AIB3fzmPlJxCiSsiIqobhhGiBmhchD+6BbiioFiPBWwVT0QNHMMIUQMkkwmIHhUCW7mA385l4MeTbBVPRA0XwwhRA9XGyxnP9itrFb/wuzPQFJRIXBERUe0wjBA1YM/2b4VWHo64nqdD9C62iieiholhhKgBU9rI8b+HOgMAthxNxl+XbkhcERFRzTGMEDVw3QPdMK6HPwDg5e1sFU9EDQ/DCFEj8NLQ9vB0VuLS9Xys+D1B6nKIiGqEYYSoEVDb22LR/cEAgJX7/sH5tFyJKyIiqj6GEaJGYmgnbwzq6HWzVfwJtoonogaDYYSokRAEAYsfCIaT0gbHk3Kw+fAVqUsiIqoWhhGiRsRHbY//Dm0HAHjn5/NI1bBVPBFZP4YRokbm8R4B6OrvgjxdKebvPM1W8URk9RhGiBoZmUzA/x7qDFu5gD1n0/HzqTSpSyIiuiOGEaJGqK2XM57u2woAsOC709AUslU8EVkvhhGiRmp6/9Zo6eGIzFwd/rfrnNTlEBFViWGEqJGys5Uj+sEQAMCXR5JwJDFL4oqIiCrHMELUiPVo6Y5HI/wAAPO2n4CulK3iyXxEUcTPp1IxbUMsfj+fIXU51IAxjBA1cnOHdkAzJyX+yczHit//kbocaiROXM3BI5/8hac/j8Oes+mYtiEWO4+nSF0WNVCC2ACu+9NqtVCr1dBoNFCpVFKXQ9Tg/HgiFdO/iINMAFp6OMFHbYfmLvbwUdvDx8UOvrf8aa+QS10uWbFUTSHe/fk8tt8MHkobGTq3UOPo5WwIArD4/mCM7xkobZFkNar7/W1jwZqISCLDQrwxqmtzbI9LQUJGHhIy8qpc1tXBFj5qe/i62N3889+/+6jt4K22g62cB1WbmnxdKT7Z/w9W/3EJRSUGAMCosOaYM6QdvFV2WPT9aWw4dAXzvz0NbVEpnu3XCoIgSFw1NRQ1PjISExODd999F8eOHUNqaip27NiBkSNHVrl8amoqZs+ejdjYWCQkJGDGjBlYunRpjYrkkRGiuhNFEZdvFOBaTuHNqQipmkJc0xQh9ea8/OK7jykRBMDTWVkxsKjt4HPzz2ZOSshk/CJqDPQGEduOXcW7v55HZq4OABAR6IZXR3RA5xYuxuVEUcT7uy9g+d6yu0Y/1bcl5g5tz0DSxNXbkZH8/HyEhoZiypQpGDVq1F2X1+l08PDwwKuvvooPPvigppsjIjMRBAFBzRwR1Myx0udFUYS2qBSpmkKk5hThmqYsoJT/PVVThNScIhTrDUjX6pCu1SE+ufJt2coFeKtvBpVbQorvzVNDvi52UNvb8ovKyv2ZcB2v/3gWZ1O1AAB/Nwe8PKw9hgR7V/jZCYKA2YPbQWVnizd/OotP9l+CtrAUb4zsBDmDKd1FncaMCIJw1yMjt+rXrx+6dOnCIyNEDZTBIOJGfnHZEZXyIys5/x5dSdUUIV1bhOrcMNjeVg4fl7KxK208ndEt0BXdAlzhqbKr/zdCd/RPZh6ifzqLPWfLrpBxtrPB8wPaYHzPACht7j6maMuRJMzbcRKiCIzo7IP3x3SBwoan9pqiBj1mRKfTQafTGR9rtVoJqyGicjKZAA9nJTyclejcovJlSvUGpOfqyk793BJSUnIKjUddbuQXo7BEj0uZ+biUmY8/Ll7H2oOJAAA/N3t0D3BDeKArugW4oY2nE0/5WEh2fjE+/O0iPv/rCkoNIuQyAePvCcCMAW3g5qio9nrGRvjDyc4GL3wVjx9OpCJfV4qPHwvn4GiqklWGkejoaCxatEjqMoioFmzkMjR3sUdzF/sqlykq0d887VOIqzmFOHlVg9gr2TiXpkVyViGSs1KMV2uo7GzQNaDsqEl4gBu6+LnwS83MdKV6bDp0Bct+uwhtUSkAYGAHT8yN7IDWnk61WueIzr5wUtrg6c+P4ffzmZi47gjWTOwGZztbc5ZOjYRVnqap7MiIn58fT9MQNXK5RSU4npSD2MtZiL2SjfjkHBTcNqjWRiYguLka3coDSqArPJ15aqc2RFHEL6fTEL3rHK7cKAAAtPd2xvwRHdG7dTOzbOPo5SxMWXcUubpShDRXY8OUiBodZaGGrUGfplEqlVAqlVKXQUQW5mxniz5tPdCnrQeAslM+Z1NzEXulLJzEXs5CulaHv5Nz8HdyDtYcKDu14+/mUBZOAt3QLdAVrT14auduTlzNwRs/nMWRy2W3CfBwVmLO4LYYHe5n1gGn3QPd8OWT92DC2iM4maLBmE8OYdPUCPioqz5yRk2PVYYRIiKg7JRPSAs1QlqoMbl3EERRxNXsQhy7kl0WUC5n43x6LpKyCpCUVWA8taO2t0VXfxd0C3RDeIAruvi5wM6Wp3aAypuWPdmnJZ7q2wpOyvr5SujUXI2vn+qJ8WsOIyEjD6NXHsLmaT0QWMWVXdT01PiTl5eXh4SEBOPjxMRExMfHw83NDf7+/pg3bx5SUlKwceNG4zLx8fHG12ZmZiI+Ph4KhQIdO3as+zsgoiZDEAT4uTnAz80BI8OaAwC0RSWIu5JdFlAul53a0RSW4Pfzmfj9fCaAskuNg31vntoJLBt74uHctI6+3qlpme8dxveYS2tPJ3zzdE+MX3MEidfzMXrVIXw+LQLtvXnqnWoxZmTfvn3o379/hfkTJ07E+vXrMWnSJFy+fBn79u37dyOV9BIICAjA5cuXq7VNXtpLRNVVojfgbKoWsZfLAsrRy1nIyNVVWC7A3QHhAa7oHuiGbgGuaNVIT+1Ut2mZpWTm6jB+zWGcS8uF2t4W6yZ3R1d/V4vXIQW9QcSxK9lwc7RFa09nqcuxiOp+f/PeNETUqJWf2ik/rXPsStmpndt/86ntbREe4IrwmwNjQxvBqZ2DCdfxRjWbllmSpqAEk9cfQVxSDhwUcnw6oZvZBsxaq6OXs7Do+9M4lVL2s2jl4YjITj4Y2skbwb6qRtsAkGGEiKgKmsISHE8qO60TeyUL8ck5xlMX5WxkZR1r23o535yc0MbLGYHuDrCx8nvz1LVpmSUUFJfiqU3H8MfF61DIZVg+LgxDgr2lLsvsruUUInrXOXz/9zUAgKNCjmK9ASX6f796/dzsMTTYG0M7+SDMz6VRHaFjGCEiqqYSvQFnrmkReyUbx65k4ejlbOMpjdsp5DK09HBEO++ykNLG0wntvJ3h5+og+ZeIuZqWWYquVI/nv4zHz6fTIJcJeHd0Z4zqWkU3vQamqESPT/Zfwsr9CSgqMUAQgLHd/TB7cDsobGTYezYDP59Kw74LGSZB2EulxJBgbwzt5I2IQDerD753wzBCRFRLoigiVVOEC+m5N6c8XEjPxcX0PBSWVH4zQTtbGVp7OpkcSWnr5YzmLvb1fgi+PpqWWUqp3oC5209i67GrAICFUR0xqXeQxFXVniiK+OlkGt766SxScgoBlI3RWRDVEZ2aqyssX1BcipgLmdh1Kg2/nc1Anq7U+JybowKDOnhhaIg3erVyt5qjWjXBMEJEZGYGg4iUnEKcT8vFhYyycHI+LRcJmXkoLjVU+hpHhRytvZzRzuvWoOIML5WyziHFEk3LLMFgELH4hzNY/+dlAMDsQW3x3H2tG9w4ijPXtFj0/WkcTizr3eKrtsO8YR0worNPtd6LrlSPPxNuYNepVOw+k47sghLjc85KGwzo4ImhnbzRt61ng+lCzDBCRGQheoOIKzfycSE9DxfTc3H+5lGUS9fzTMYG3MrZzgbtvJzR5uZRlPK/N3NSVOuLy1JNyyxFFEUs3XMRH/52EQDwxL1BeHlYhwYRSLLyi/Her+ex5UgSDGJZ75an+7bC031b1To0lOoNOJKYhV2n0vDL6TSTK8LsbeXo184DQzt54772nlbdYp9hhIhIYiV6Ay5fLwspZQGl7LTP5RsF0Fdxa2NXB9sKp3raejnD9eaYj2s5hXj3l/PYYcGmZZa05kAiXv/hDADgkW5+eGtUiNWGqxK9AZsOXcHSPReMp8eGd/bBy8M63PHeTDVlMIg4npyNXSfTsOtUmvH0D1A2hql3a3dEdvLBoI5exs+JtWAYISKyUrrSsjsW3z4mJSmroMIlx+U8nJVo5eFocuWPJZuWWdLXscmYu+0EDCIwPMQHHzzSBQob6xrIGXMhE4t/OIOEjDwAQEcfFV6L6ogeLd3rdbuiKOL0NS12nUrFrlNpuJSZb3xOLhPQI8gNkZ28MSTYG54q6e/ZxDBCRNTAFBbr8U9mXoUxKbf+TxiQtmmZpew6mYoZW46jRC+ib1sPrHo83CrGSVy+no83fjyLPWfTAZQNMp0zuB0e6S7N6bGL6bnYdSoNP59Kw5mb/WQAQBCArv6uxmDi5+Zg8doAhhEiokYjT1eKhIyyoyfeKjvc26ZZgxhLUVcxFzLx1KZjKCzRo3ugK9ZM6g6VROMj8nSl+GhvAtYeSESx3gAbmYAJPQPx/IA2UDtYx5iNKzfy8fOpNPx8Og3Hk3JMnuvUXIXITj4YEuxt0SusGEaIiKjBO3YlC5PWHUVuUSmCfVXYMCUCzZwsd18hg0HE9uMpePvnc8beM/e2aYbXojpadUv3VE0hfjlVNsbk6OUs3DpEqY2nU9kRk07e6OhTv91fGUaIiKhROH1Ng4lrj+B6XjFaejji86k9LDJOJi4pG4u+P4O/k3MAAIHuDpg/oiPua+/ZoI5MXc/TYfeZdOw6lYY/E66j9JZk4u/mYAwmXVqYv/srwwgRETUalzLz8Phnh3FNU4TmLvbYNDUCLT3q53RDurYIb+86h+03r1hyUtrg/+5rjUm9Axtk47FbaQpL8NvZdPx8Kg37L2RCd0t/nJeGtscz/VqZdXsMI0RE1Kik5BRi/GeHcel6Ppo5KbBxSg909DXfd0JRiR5rDiRixe8JKCgu67T7cHgLvDi0HTydpb8yxdzydaXYdz4TP59Ow96z6dj2bC+09zbvdyzDCBERNTrX83SYsOYIzqRqobKzwbrJ3REe4FandYqiiF/PpOPNH88iKausk22YvwsWRgUj1M/FDFVbv6ISPZQ2MrOffmIYISKiRklTWIKp648i9ko27G3lWD0hHPe28ajVui6k52LR96dxMOEGgLIb1c2NbI+RXZo3qHEh1ophhIiIGq2C4lI8/XkcYi5kQiGXYdmjXTC0k0+1X59TUIwPdl/A54eToDeIUNjI8OS9LfFMv1ZwbASdbK0FwwgRETVqxaUGzPzqOH46mQaZALz9UGc83M3vjq8p1Rvw5ZEkLNl9ATk3b0Q3NNgbLw/rAH93aRqDNWbV/f5m/CMiogZJYSPD8ke7wkl5Al/HXsWLW08gt6gUU/4TVOnyf/5zHYu/P4NzabkAgHZezlgQ1bDucNxYMYwQEVGDJZcJePuhzlDZ2eKzA4lY/MMZaItK8PyANsYxH8lZBXjzx7P4+XQaAEBtb4vZg9tiXIQ/bOTWdc+bpophhIiIGjRBEPDK8A5Q29tiye4LWLrnIrSFpZg9uC1W7vsHq/+4hOJSA2QC8Pg9AXhhYFuru7ttU8cxI0RE1GisP5iIhd+fAQA4KOTGfiG9WrljQVRHs/fRoDvjmBEiImpyJvUOgrOdLV7c+jcKivXwc7PHK8M6YkiwFy/VtWIMI0RE1Kg8FN4Cfm4O+CczDw+GNYedbcNu4d4UMIwQEVGjExHkhoigunVmJcvhMGIiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSNQ4jMTExiIqKgq+vLwRBwM6dO+/6mn379qFr165QKpVo3bo11q9fX4tSiYiIqDGqcRjJz89HaGgoVqxYUa3lExMTMXz4cPTv3x/x8fGYOXMmpk2bhl9++aXGxRIREVHjU+MOrJGRkYiMjKz28qtWrUJQUBCWLFkCAOjQoQMOHDiADz74AEOGDKnp5omIiKiRqfcxI4cOHcLAgQNN5g0ZMgSHDh2q8jU6nQ5ardZkIiIiosap3sNIWloavLy8TOZ5eXlBq9WisLCw0tdER0dDrVYbJz8/v/ouk4iIiCRilVfTzJs3DxqNxjglJydLXRIRERHVk3q/a6+3tzfS09NN5qWnp0OlUsHe3r7S1yiVSiiVSuNjURQBgKdriIiIGpDy7+3y7/Gq1HsY6dmzJ3766SeTebt370bPnj2rvY7c3FwA4OkaIiKiBig3NxdqtbrK52scRvLy8pCQkGB8nJiYiPj4eLi5ucHf3x/z5s1DSkoKNm7cCAB4+umn8dFHH+G///0vpkyZgr179+Lrr7/Gjz/+WO1t+vr6Ijk5Gc7OzhAEoaYlV0mr1cLPzw/JyclQqVRmW29jxf1VfdxX1cd9VX3cV9XHfVV99bmvRFFEbm4ufH1977hcjcNIbGws+vfvb3w8a9YsAMDEiROxfv16pKamIikpyfh8UFAQfvzxR7zwwgv48MMP0aJFC3z22Wc1uqxXJpOhRYsWNS212lQqFT+sNcD9VX3cV9XHfVV93FfVx31VffW1r+50RKRcjcNIv3797njup7Luqv369cPx48druikiIiJqAqzyahoiIiJqOpp0GFEqlXjttddMrtyhqnF/VR/3VfVxX1Uf91X1cV9VnzXsK0G82/U2RERERPWoSR8ZISIiIukxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpJUkw4jK1asQGBgIOzs7NCjRw8cOXJE6pKsTnR0NLp37w5nZ2d4enpi5MiROH/+vNRlNQj/+9//IAgCZs6cKXUpViklJQWPP/443N3dYW9vj5CQEMTGxkpdllXS6/WYP38+goKCYG9vj1atWuH111+/683HmoKYmBhERUXB19cXgiBg586dJs+LoogFCxbAx8cH9vb2GDhwIC5evChNsRK7074qKSnBSy+9hJCQEDg6OsLX1xcTJkzAtWvXLFJbkw0jX331FWbNmoXXXnsNcXFxCA0NxZAhQ5CRkSF1aVZl//79mD59Ov766y/s3r0bJSUlGDx4MPLz86UuzaodPXoUn3zyCTp37ix1KVYpOzsbvXv3hq2tLXbt2oUzZ85gyZIlcHV1lbo0q/T2229j5cqV+Oijj3D27Fm8/fbbeOedd7B8+XKpS5Ncfn4+QkNDsWLFikqff+edd7Bs2TKsWrUKhw8fhqOjI4YMGYKioiILVyq9O+2rgoICxMXFYf78+YiLi8P27dtx/vx53H///ZYpTmyiIiIixOnTpxsf6/V60dfXV4yOjpawKuuXkZEhAhD3798vdSlWKzc3V2zTpo24e/dusW/fvuLzzz8vdUlW56WXXhL/85//SF1GgzF8+HBxypQpJvNGjRolPvbYYxJVZJ0AiDt27DA+NhgMore3t/juu+8a5+Xk5IhKpVL88ssvJajQety+rypz5MgREYB45cqVeq+nSR4ZKS4uxrFjxzBw4EDjPJlMhoEDB+LQoUMSVmb9NBoNAMDNzU3iSqzX9OnTMXz4cJPPF5n67rvv0K1bNzz88MPw9PREWFgYPv30U6nLslq9evXCb7/9hgsXLgAA/v77bxw4cACRkZESV2bdEhMTkZaWZvJvUa1Wo0ePHvxdXw0ajQaCIMDFxaXet1XjG+U1BtevX4der4eXl5fJfC8vL5w7d06iqqyfwWDAzJkz0bt3b3Tq1EnqcqzSli1bEBcXh6NHj0pdilW7dOkSVq5ciVmzZuHll1/G0aNHMWPGDCgUCkycOFHq8qzO3LlzodVq0b59e8jlcuj1erz55pt47LHHpC7NqqWlpQFApb/ry5+jyhUVFeGll17Co48+apG7HjfJMEK1M336dJw6dQoHDhyQuhSrlJycjOeffx67d++GnZ2d1OVYNYPBgG7duuGtt94CAISFheHUqVNYtWoVw0glvv76a2zevBlffPEFgoODER8fj5kzZ8LX15f7i8yupKQEY8aMgSiKWLlypUW22SRP0zRr1gxyuRzp6ekm89PT0+Ht7S1RVdbtueeeww8//IDff/8dLVq0kLocq3Ts2DFkZGSga9eusLGxgY2NDfbv349ly5bBxsYGer1e6hKtho+PDzp27Ggyr0OHDkhKSpKoIuv24osvYu7cuRg7dixCQkIwfvx4vPDCC4iOjpa6NKtW/vucv+urrzyIXLlyBbt377bIURGgiYYRhUKB8PBw/Pbbb8Z5BoMBv/32G3r27ClhZdZHFEU899xz2LFjB/bu3YugoCCpS7JaAwYMwMmTJxEfH2+cunXrhsceewzx8fGQy+VSl2g1evfuXeES8QsXLiAgIECiiqxbQUEBZDLTX9dyuRwGg0GiihqGoKAgeHt7m/yu12q1OHz4MH/XV6I8iFy8eBF79uyBu7u7xbbdZE/TzJo1CxMnTkS3bt0QERGBpUuXIj8/H5MnT5a6NKsyffp0fPHFF/j222/h7OxsPM+qVqthb28vcXXWxdnZucJYGkdHR7i7u3OMzW1eeOEF9OrVC2+99RbGjBmDI0eOYPXq1Vi9erXUpVmlqKgovPnmm/D390dwcDCOHz+O999/H1OmTJG6NMnl5eUhISHB+DgxMRHx8fFwc3ODv78/Zs6ciTfeeANt2rRBUFAQ5s+fD19fX4wcOVK6oiVyp33l4+OD0aNHIy4uDj/88AP0er3x972bmxsUCkX9Flfv1+tYseXLl4v+/v6iQqEQIyIixL/++kvqkqwOgEqndevWSV1ag8BLe6v2/fffi506dRKVSqXYvn17cfXq1VKXZLW0Wq34/PPPi/7+/qKdnZ3YsmVL8ZVXXhF1Op3UpUnu999/r/R31MSJE0VRLLu8d/78+aKXl5eoVCrFAQMGiOfPn5e2aIncaV8lJiZW+fv+999/r/faBFFkCz8iIiKSTpMcM0JERETWg2GEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESS+n9uGVAnrLOvyQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import timm  # si vous utilisez ConvNeXt ou un autre modèle timm\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 📦 Dataset pour test (pas de labels dans les noms)\n",
        "# --------------------------------------------------\n",
        "class ImageDatasetWithoutLabels(Dataset):\n",
        "    def __init__(self, image_dir, image_filenames, transform=None, img_size=(300,300)):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_filenames = image_filenames\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.image_dir, filename)\n",
        "\n",
        "        # Ouverture, conversion en RGB, resize\n",
        "        image = Image.open(img_path).convert(\"RGB\").resize(self.img_size)\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, filename\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🔧 Préparation\n",
        "# --------------------------------------------------\n",
        "image_dir = \"/content/drive/MyDrive/datatest\"\n",
        "# Chemin vers votre dossier contenant les images de test\n",
        "\n",
        "test_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
        "print(f\"Nombre d'images trouvées: {len(test_files)}\")\n",
        "\n",
        "# Exemple de transformations (à ajuster selon votre entraînement)\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(300, 300),  # même taille que lors de l'entraînement\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Device:\", device)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🔍 Chargement du modèle entraîné\n",
        "# --------------------------------------------------\n",
        "NUM_CLASSES = 9\n",
        "\n",
        "# Remplacez 'convnext_base' par l'architecture exacte\n",
        "# utilisée lors de l'entraînement si nécessaire\n",
        "model = timm.create_model(\n",
        "    'convnext_base',  # ou convnext_tiny, convnext_small, etc.\n",
        "    pretrained=False, # on charge nos poids persos, donc pas les poids ImageNet\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n",
        "\n",
        "# Charger les poids depuis votre fichier\n",
        "# (ici on suppose que vous avez sauvegardé sous \"model_finetuned_mixup.pth\")\n",
        "state_dict = torch.load(\"model_finetuned_mixup.pth\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"✅ Modèle chargé et prêt pour l'inférence\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 📤 Dataset et DataLoader\n",
        "# --------------------------------------------------\n",
        "test_dataset = ImageDatasetWithoutLabels(\n",
        "    image_dir=image_dir,\n",
        "    image_filenames=test_files,\n",
        "    transform=val_transform,\n",
        "    img_size=(300, 300)  # Ajustez si votre modèle attend 224x224 ou autre\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "print(\"✅ Test loader prêt\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 🧠 Inférence + création de la soumission\n",
        "# --------------------------------------------------\n",
        "submission_rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Prédictions: argmax des logits\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        for filename, pred in zip(filenames, preds):\n",
        "            # On enlève l'extension .jpg pour faire un ID plus propre\n",
        "            img_id = os.path.splitext(filename)[0]\n",
        "            submission_rows.append((img_id, pred))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 💾 Export CSV\n",
        "# --------------------------------------------------\n",
        "submission_df = pd.DataFrame(submission_rows, columns=[\"idx\", \"gt\"])\n",
        "submission_df.to_csv(\"submission_convnext1.csv\", index=False)\n",
        "print(\"✅ Fichier de prédiction généré : submission_convnext.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53tTLiiGV7CB",
        "outputId": "418a139e-4100-434b-c1ac-f7597dde5cd7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'images trouvées: 1344\n",
            "✅ Device: cuda\n",
            "✅ Modèle chargé et prêt pour l'inférence\n",
            "✅ Test loader prêt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [01:20<00:00,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fichier de prédiction généré : submission_convnext.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "645f84f6ee034174a95d1e783b03821c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fb4eb39a32d4ccc8a6a746e9bad42b9",
              "IPY_MODEL_778e6e7ea2134030866c8c4fcb90799e",
              "IPY_MODEL_e7037a9471014fbe9e769cfc484e0199"
            ],
            "layout": "IPY_MODEL_a724eb97b3b14c45b7b947e8cecc6e39"
          }
        },
        "0fb4eb39a32d4ccc8a6a746e9bad42b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_490d0277e1ae401796f247b786ec370f",
            "placeholder": "​",
            "style": "IPY_MODEL_76ad502da13f465b82c80a0d48748037",
            "value": "model.safetensors: 100%"
          }
        },
        "778e6e7ea2134030866c8c4fcb90799e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd81b1252d294237bdbe822eba1ef8e0",
            "max": 354400320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9adf23f53e9447583dbcfff620788dd",
            "value": 354400320
          }
        },
        "e7037a9471014fbe9e769cfc484e0199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f500f37e9ff46009f1bcf7977cf7973",
            "placeholder": "​",
            "style": "IPY_MODEL_55ea3ad9d06e47eca8e2b259f519b829",
            "value": " 354M/354M [00:01&lt;00:00, 232MB/s]"
          }
        },
        "a724eb97b3b14c45b7b947e8cecc6e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490d0277e1ae401796f247b786ec370f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ad502da13f465b82c80a0d48748037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd81b1252d294237bdbe822eba1ef8e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9adf23f53e9447583dbcfff620788dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f500f37e9ff46009f1bcf7977cf7973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ea3ad9d06e47eca8e2b259f519b829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}