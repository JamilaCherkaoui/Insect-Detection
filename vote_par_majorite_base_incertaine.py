# -*- coding: utf-8 -*-
"""vote_par_majorite_base_incertaine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5Bw8flliHH-2NZl3WItIqFOguHJc3yK
"""

import os
import re
import csv
import os
import re
import shutil
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

"""# **Générateur**"""

import os
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Répertoire où se trouvent les images "sûres"
sure_directory = '/content/drive/MyDrive/ChallengeIAM2/base_sure'

# CSV contenant les métadonnées
csv_file_path = '/content/drive/MyDrive/ChallengeIAM2/metadonnees.csv'

###############################################################################
# 1) Charger le CSV en DataFrame
###############################################################################
df = pd.read_csv(csv_file_path)
if not all(col in df.columns for col in ['filename', 'vote']):
    raise ValueError("Le CSV doit contenir les colonnes 'filename' et 'vote'.")

###############################################################################
# 2) Filtrer le DataFrame pour ne garder que les lignes où les 4 votes sont identiques
###############################################################################
def is_sure_vote(vote_str):
    vote_parts = vote_str.split('_')
    return len(set(vote_parts)) == 1  # True si tous les votes sont identiques

df_sure = df[df['vote'].apply(is_sure_vote)].copy()  # on crée un DF séparé

###############################################################################
# 3) Déterminer le label final (valeur unique) et construire le chemin vers base_sure
###############################################################################
def compute_label_sure(vote_str):
    # On sait déjà qu'ils sont identiques => on renvoie directement vote_parts[0]
    return vote_str.split('_')[0]

df_sure['label_final'] = df_sure['vote'].apply(compute_label_sure)

def get_sure_image_path(row):
    filename = row['filename']
    return os.path.join(sure_directory, filename)

df_sure['image_path'] = df_sure.apply(get_sure_image_path, axis=1)

# Vérification rapide
print("Aperçu du DF 'df_sure':")
print(df_sure.head())

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Supposons que df_sure contient déjà :
# - "image_path" : chemin complet vers chaque image (dans base_sure)
# - "label_final" : label (ex: "5", "6", etc.)

# Séparation du DataFrame en 80% train et 20% validation
df_train, df_val = train_test_split(
    df_sure,
    test_size=0.2,
    stratify=df_sure['label_final'],
    random_state=42
)

###############################################################################
# Générateur pour l'entraînement (avec data augmentation)
###############################################################################
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode='nearest',
    rescale=1./255
)

train_generator = datagen.flow_from_dataframe(
    dataframe=df_train,
    x_col='image_path',      # chemin complet de l'image (dans base_sure)
    y_col='label_final',     # ex: "5", "6", etc.
    target_size=(224, 224),
    color_mode='rgb',
    class_mode='categorical',  # ou 'sparse' si vous préférez un label entier
    batch_size=32,
    shuffle=True,
    seed=42
)

print("Nombre d'images (train) :", train_generator.n)
print("Classes détectées :", train_generator.class_indices)

###############################################################################
# Générateur pour la validation (sans data augmentation)
###############################################################################
val_datagen = ImageDataGenerator(
    rescale=1./255  # Seul le rescale est appliqué
)

val_generator = val_datagen.flow_from_dataframe(
    dataframe=df_val,
    x_col='image_path',
    y_col='label_final',
    target_size=(224, 224),
    color_mode='rgb',
    class_mode='categorical',
    batch_size=32,
    shuffle=False
)

print("Nombre d'images (validation) :", val_generator.n)

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Suppose qu'on a déjà df_sure, etc.
# Crée un "viz_generator" sans shuffle pour aligner l'ordre batch <-> noms de fichiers
viz_datagen = ImageDataGenerator(rescale=1./255)
viz_generator = viz_datagen.flow_from_dataframe(
    dataframe=df_sure,
    x_col="image_path",
    y_col="label_final",
    target_size=(224, 224),
    class_mode="categorical",  # ou "sparse", selon ton cas
    batch_size=32,
    shuffle=False
)

# Choisir un batch à visualiser (par exemple, le premier batch)
batch_index = 0
images_batch, labels_batch = viz_generator[batch_index]

# Indices de fichiers pour ce batch
start = batch_index * viz_generator.batch_size
end = start + images_batch.shape[0]
batch_filenames = viz_generator.filenames[start:end]

# Convertir labels si class_mode="categorical" (one-hot)
if viz_generator.class_mode == "categorical":
    labels_idx = np.argmax(labels_batch, axis=1)
elif viz_generator.class_mode == "sparse":
    labels_idx = labels_batch.astype(int)
else:
    labels_idx = labels_batch

# Mapping indice -> nom de classe
class_indices = viz_generator.class_indices
idx_to_class = {v: k for k, v in class_indices.items()}

# Nombre d'images à afficher (jusqu'à 9)
n_display = min(9, images_batch.shape[0])

plt.figure(figsize=(10, 10))

for i in range(n_display):
    ax = plt.subplot(3, 3, i + 1)
    ax.imshow(images_batch[i])
    ax.axis("off")

    # Label (ex "5", "6", etc.)
    class_id = labels_idx[i]
    class_label = idx_to_class[class_id]

    # Nom de fichier
    filename = os.path.basename(batch_filenames[i])

    # Afficher dans le titre : Fichier + Label
    # On réduit un peu la taille pour éviter les titres trop longs
    ax.set_title(f"\nLabel: {class_label}", fontsize=8)

plt.tight_layout()
plt.show()

# Optionnel: imprimer dans la console un récap
print("Vérification console :")
for i in range(n_display):
    class_label = idx_to_class[labels_idx[i]]
    filename = os.path.basename(batch_filenames[i])
    print(f"{i+1}. Fichier: {filename}  =>  Label: {class_label}")

"""# **Modèle** **1**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import f1_score

# Définir les dimensions des images
IMG_HEIGHT = 224
IMG_WIDTH = 224

# Utiliser len() pour obtenir le nombre de classes
NUM_CLASSES = len(train_generator.class_indices)

# Chargement de MobileNetV2 sans la tête de classification (include_top=False)
base_model = MobileNetV2(
    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),
    include_top=False,
    weights='imagenet'
)

# Geler les couches de base
for layer in base_model.layers:
    layer.trainable = False

# Ajouter une tête de classification personnalisée
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(NUM_CLASSES, activation='softmax')
])

# Compilation du modèle
model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
# --- Entraînement du modèle ---
EPOCHS = 30

history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator
)

import matplotlib.pyplot as plt

# Tracer la courbe de la loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss entraînement')
plt.plot(history.history['val_loss'], label='Loss validation')
plt.title('Evolution de la Loss')
plt.xlabel('Époch')
plt.ylabel('Loss')
plt.legend()

# Tracer la courbe de l'accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Accuracy entraînement')
plt.plot(history.history['val_accuracy'], label='Accuracy validation')
plt.title('Evolution de l\'Accuracy')
plt.xlabel('Époch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

import os
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Chemin vers le dossier contenant les images incertaines
incertain_directory = '/content/drive/MyDrive/ChallengeIAM2/base_incertaine'

# Liste des fichiers de la base incertaine
test_filenames = sorted(os.listdir(incertain_directory))

# Création d'un DataFrame minimal pour le générateur de test
df_test = pd.DataFrame({
    'filename': test_filenames,
    'image_path': [os.path.join(incertain_directory, f) for f in test_filenames]
})

# Générateur de test : aucune augmentation, juste le rescale
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_dataframe(
    dataframe=df_test,
    x_col='image_path',
    y_col=None,           # Pas de labels pour le test
    target_size=(224, 224),
    color_mode='rgb',
    class_mode=None,      # Aucun label fourni
    batch_size=32,
    shuffle=False
)

# Prédiction sur le générateur de test
preds_proba = model.predict(test_generator)
preds_classes = np.argmax(preds_proba, axis=1)

# Conversion des indices en labels à l'aide du mapping de train_generator
inv_class_indices = {v: k for k, v in train_generator.class_indices.items()}
predicted_labels = [inv_class_indices[idx] for idx in preds_classes]

# Construction d'un DataFrame de soumission en retirant l'extension ".jpg" des noms de fichiers
submission_df = pd.DataFrame({
    'idx': [os.path.splitext(f)[0] for f in test_filenames],
    'gt': predicted_labels
})

# Sauvegarde du fichier CSV de soumission
submission_csv_path = '/content/drive/MyDrive/ChallengeIAM2/preds_labels_scientistn5.csv'
submission_df.to_csv(submission_csv_path, index=False)

print("Prédictions sur la base incertaine :")
print(submission_df.head())

"""# **Changement de labels**"""

def extract_label_between(filename):
    # Split the filename at "-" and return the token between the first and second dash.
    parts = filename.split("-")
    if len(parts) < 3:
        return ""
    return parts[1]

# Update the "gt" column: new gt = (substring between first and second dash) + "_" + original gt value
submission_df["gt"] = submission_df.apply(
    lambda row: f"{extract_label_between(row['idx'])}_{row['gt']}",
    axis=1
)

print(submission_df)

from collections import Counter

def compute_majority_info(vote_string):

    votes = vote_string.split("_")
    counter = Counter(votes)
    # Get the most common vote(s)
    most_common = counter.most_common()
    if not most_common:
        return None, []
    majority_vote, freq = most_common[0]
    # In case of tie, most_common() returns them in arbitrary order (order of first appearance).
    # We'll simply take the first one.
    agreeing_positions = [i+1 for i, v in enumerate(votes) if v == majority_vote]
    return majority_vote, agreeing_positions

# Apply the function to create new columns
def majority_str(vote_string):
    # Returns a string representation of the agreeing positions
    maj, positions = compute_majority_info(vote_string)
    if positions:
        return "_".join(map(str, positions))
    else:
        return ""

submission_df["majority_vote"] = submission_df["gt"].apply(lambda s: compute_majority_info(s)[0])
submission_df["agreeing_people"] = submission_df["gt"].apply(lambda s: majority_str(s))

print(submission_df)

def compute_agreeing_positions(vote_string):

    votes = vote_string.split("_")
    counts = Counter(votes)
    max_freq = max(counts.values())
    # Identify all vote values that occur with max frequency.
    majority_candidates = {vote for vote, freq in counts.items() if freq == max_freq}
    # Get 1-indexed positions where the vote is one of the majority candidates.
    positions = [str(i + 1) for i, vote in enumerate(votes) if vote in majority_candidates]
    return "_".join(positions)

# Example usage:
vote_string = "8_8_0_0_2"
result = compute_agreeing_positions(vote_string)
print(result)

import pandas as pd
from collections import Counter

def compute_agreeing_positions(vote_string):

    votes = vote_string.split("_")
    counts = Counter(votes)
    max_freq = max(counts.values())
    # All vote values that have the maximum frequency
    majority_candidates = {vote for vote, freq in counts.items() if freq == max_freq}
    # Collect positions (1-indexed) for votes in the majority
    positions = [str(i+1) for i, vote in enumerate(votes) if vote in majority_candidates]
    return "_".join(positions)

# Assuming your DataFrame is named soumission_df and has a column "gt"
submission_df["agreed_positions"] = submission_df["gt"].apply(compute_agreeing_positions)

print(submission_df)

all_positions = []
for pos_str in submission_df["agreed_positions"]:
    # Ensure empty strings are skipped
    if pos_str:
        all_positions.extend(pos_str.split("_"))
global_counts = Counter(all_positions)
winning_position_str, _ = global_counts.most_common(1)[0]
winning_position = int(winning_position_str)
print("Global winning position (1-indexed):", winning_position)

# 2. For each row, if len(agreeing_people) < len(agreed_positions), get the vote at winning_position.
def compute_winning_vote(row):
    # Convert the underscore-separated strings to lists.
    agreeing = row["agreeing_people"].split("_") if row["agreeing_people"] != "" else []
    agreed = row["agreed_positions"].split("_") if row["agreed_positions"] != "" else []
    # If the number of agreeing votes is less than the number of agreed positions:
    if len(agreeing) < len(agreed):
        # Split the original vote string.
        votes = row["gt"].split("_")
        # Check if the winning_position is within range.
        if winning_position <= len(votes):
            # Return the vote at that position (convert from 1-indexed to 0-indexed).
            return votes[winning_position - 1]
        else:
            return ""
    else:
        return ""

# Create the new column "winning_vote"
submission_df["winning_vote"] = submission_df.apply(compute_winning_vote, axis=1)

# Display the DataFrame
print(submission_df)

submission_csv_path = '/content/drive/MyDrive/ChallengeIAM2/preds_labels_scientistn5.csv'
submission_df.to_csv(submission_csv_path, index=False)

df = pd.read_csv('/content/drive/MyDrive/ChallengeIAM2/preds_labels_scientistn5.csv')

df

df["majority_vote"] = df.apply(
    lambda row: row["winning_vote"] if pd.notnull(row["winning_vote"]) and row["winning_vote"] != "" else row["majority_vote"],
    axis=1
)

def replace_token(idx_string, majority_vote):
    """
    Given an idx string in the format "prefix-token-rest" where token is the part between
    the first and second dashes (e.g. "1_6_6_1"), replace that token with the majority_vote (as an integer)
    repeated as many times as there are parts in the original token.
    """
    parts = idx_string.split("-")
    if len(parts) < 3:
        return idx_string  # not in expected format
    token = parts[1]  # token to replace (e.g., "1_6_6_1")
    token_parts = token.split("_")
    count = len(token_parts)
    # Convert majority_vote to integer and then to string, so 0.0 becomes "0"
    new_token = "_".join([str(int(majority_vote))] * count)
    parts[1] = new_token
    return "-".join(parts)

# Create a new column "new_idx" without modifying the original "idx" column.
df["new_idx"] = df.apply(
    lambda row: replace_token(row["idx"], row["majority_vote"])
    if pd.notnull(row["majority_vote"]) and row["majority_vote"] != "" else row["idx"],
    axis=1
)

print(df[["idx", "new_idx"]])

src_dir = '/content/drive/MyDrive/ChallengeIAM2/base_incertaine'
# Destination directory for renamed files
dest_dir = '/content/drive/MyDrive/ChallengeIAM2/base_incertaine_votedmaj'

# Create destination directory if it doesn't exist
os.makedirs(dest_dir, exist_ok=True)

def find_source_file(idx_without_ext, src_dir, extensions=[".jpg", ".jpeg", ".png"]):
    """
    Given a base filename (without extension) and a list of extensions,
    returns the first file that exists in src_dir with that base name.
    """
    for ext in extensions:
        candidate = os.path.join(src_dir, idx_without_ext + ext)
        if os.path.exists(candidate):
            return candidate
    return None

# Iterate over each row in the DataFrame and copy files with new names.
for i, row in df.iterrows():
    original_idx = str(row["idx"]).strip()   # e.g., without extension
    new_idx = str(row["new_idx"]).strip()      # new name (without extension)

    # Try to find the corresponding file in the source directory
    src_file = find_source_file(original_idx, src_dir)
    if src_file is None:
        print(f"Could not find file for idx: {original_idx} in {src_dir}")
        continue

    # Get the extension from the found file
    _, ext = os.path.splitext(src_file)
    dest_file = os.path.join(dest_dir, new_idx + ext)

    try:
        shutil.copy2(src_file, dest_file)
        print(f"Copied: {src_file} -> {dest_file}")
    except Exception as e:
        print(f"Error copying {src_file} to {dest_file}: {e}")

